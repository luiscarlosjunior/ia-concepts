# Estrat√©gias de Evolu√ß√£o (Evolution Strategies - ES)

As **Estrat√©gias de Evolu√ß√£o** (Evolution Strategies - ES) s√£o uma fam√≠lia de algoritmos evolutivos desenvolvidos na Alemanha nos anos 1960 por Ingo Rechenberg e Hans-Paul Schwefel. Diferentemente dos Algoritmos Gen√©ticos, as ES foram projetadas especificamente para otimiza√ß√£o de par√¢metros cont√≠nuos e s√£o conhecidas por sua capacidade de **auto-adapta√ß√£o** de par√¢metros de controle.

![Evolution Strategies Concept](../../images/evolution_strategies_concept.png)

As ES s√£o particularmente eficazes em problemas de otimiza√ß√£o num√©rica complexa, ruidosa e de alta dimens√£o, sendo amplamente utilizadas em rob√≥tica, engenharia, reinforcement learning e design de sistemas complexos.

---

## **1. üéØ Fundamentos Te√≥ricos**

### **1.1 Conceito Central**

As Estrat√©gias de Evolu√ß√£o se baseiam em princ√≠pios fundamentais:

1. **Representa√ß√£o Real:** Trabalha diretamente com valores reais (n√£o bin√°rios)
2. **Auto-adapta√ß√£o:** Par√¢metros de muta√ß√£o evoluem junto com a solu√ß√£o
3. **Sele√ß√£o Determin√≠stica:** Baseada apenas em ranking de fitness
4. **Muta√ß√£o Gaussiana:** Principal operador de varia√ß√£o

**Intui√ß√£o:**
> "Assim como na natureza, n√£o apenas as solu√ß√µes evoluem, mas tamb√©m a forma como elas se transformam - a estrat√©gia de evolu√ß√£o em si evolui."

### **1.2 Nota√ß√£o das ES**

As ES s√£o comumente descritas pela nota√ß√£o **(Œº/œÅ +/,  Œª)-ES:**

- **Œº (mi):** N√∫mero de pais selecionados para reprodu√ß√£o
- **œÅ (rho):** N√∫mero de pais que contribuem para criar um filho
- **Œª (lambda):** N√∫mero de filhos gerados
- **+:** Sele√ß√£o geracional (pais + filhos competem)
- **,:** Sele√ß√£o n√£o-geracional (apenas filhos competem)

#### **Exemplos Comuns:**

```
(1+1)-ES: 1 pai, 1 filho, melhor sobrevive
(Œº+Œª)-ES: Œº pais, Œª filhos, melhores Œº sobrevivem (elitista)
(Œº,Œª)-ES: Œº pais, Œª filhos, melhores Œº entre filhos sobrevivem (n√£o-elitista)
          Requer Œª ‚â• Œº
```

### **1.3 Diferen√ßas dos Algoritmos Gen√©ticos**

| Aspecto | Algoritmos Gen√©ticos | Evolution Strategies |
|---------|---------------------|---------------------|
| **Origem** | EUA (John Holland) | Alemanha (Rechenberg, Schwefel) |
| **Representa√ß√£o** | Bin√°ria/Inteira | Real/Cont√≠nua |
| **Muta√ß√£o** | Secund√°ria | Prim√°ria |
| **Crossover** | Prim√°rio | Secund√°rio/Opcional |
| **Sele√ß√£o** | Probabil√≠stica (roleta, torneio) | Determin√≠stica (ranking) |
| **Auto-adapta√ß√£o** | Rara | Fundamental |
| **Aplica√ß√£o** | Problemas combinat√≥rios | Otimiza√ß√£o num√©rica |

---

## **2. üîß Algoritmo das Estrat√©gias de Evolu√ß√£o**

### **2.1 (1+1)-ES: A ES Mais Simples**

A forma mais b√°sica com um pai e um filho:

```
üöÄ 1. INICIALIZA√á√ÉO
   ‚îú‚îÄ‚îÄ x ‚Üê solu√ß√£o inicial aleat√≥ria
   ‚îú‚îÄ‚îÄ œÉ ‚Üê step-size inicial (desvio padr√£o)
   ‚îî‚îÄ‚îÄ Avaliar f(x)

üîÑ 2. LOOP EVOLUTIVO (para cada gera√ß√£o):
   ‚îÇ
   ‚îú‚îÄ‚îÄ üß¨ MUTA√á√ÉO
   ‚îÇ   ‚îú‚îÄ‚îÄ x' ‚Üê x + N(0, œÉ¬≤I)    # I = matriz identidade
   ‚îÇ   ‚îî‚îÄ‚îÄ Avaliar f(x')
   ‚îÇ
   ‚îú‚îÄ‚îÄ üéØ SELE√á√ÉO
   ‚îÇ   SE f(x') ‚â§ f(x):  # Para minimiza√ß√£o
   ‚îÇ       x ‚Üê x'        # Aceita filho
   ‚îÇ   SEN√ÉO:
   ‚îÇ       manter x      # Mant√©m pai
   ‚îÇ
   ‚îî‚îÄ‚îÄ üîß ADAPTAR STEP-SIZE (Regra 1/5)
       ‚îú‚îÄ‚îÄ Contar sucessos nas √∫ltimas n gera√ß√µes
       ‚îú‚îÄ‚îÄ SE taxa_sucesso > 1/5:
       ‚îÇ       œÉ ‚Üê œÉ / c    # Aumentar œÉ (c < 1)
       ‚îú‚îÄ‚îÄ SE taxa_sucesso < 1/5:
       ‚îÇ       œÉ ‚Üê œÉ * c    # Diminuir œÉ
       ‚îî‚îÄ‚îÄ c ‚âà 0.82 (t√≠pico)

üèÜ 3. RETORNAR melhor solu√ß√£o
```

**Regra 1/5 de Rechenberg:**
> "Para converg√™ncia √≥tima, a taxa de muta√ß√µes bem-sucedidas deve ser aproximadamente 1/5."

### **2.2 (Œº+Œª)-ES: Estrat√©gia Geracional**

```
üöÄ 1. INICIALIZA√á√ÉO
   ‚îú‚îÄ‚îÄ Gerar popula√ß√£o P de Œº indiv√≠duos
   ‚îÇ   Cada indiv√≠duo: (x, œÉ) onde x = solu√ß√£o, œÉ = step-sizes
   ‚îî‚îÄ‚îÄ Avaliar todos os indiv√≠duos

üîÑ 2. LOOP EVOLUTIVO:
   ‚îÇ
   ‚îú‚îÄ‚îÄ üß¨ CRIAR Œª FILHOS
   ‚îÇ   PARA i = 1 at√© Œª:
   ‚îÇ       ‚îú‚îÄ‚îÄ Selecionar œÅ pais aleatoriamente
   ‚îÇ       ‚îú‚îÄ‚îÄ Recombina√ß√£o (opcional):
   ‚îÇ       ‚îÇ   x_filho ‚Üê recombinar(pais)
   ‚îÇ       ‚îÇ   œÉ_filho ‚Üê recombinar(œÉ_pais)
   ‚îÇ       ‚îú‚îÄ‚îÄ Muta√ß√£o:
   ‚îÇ       ‚îÇ   œÉ'_filho ‚Üê œÉ_filho * exp(œÑ¬∑N(0,1) + œÑ'¬∑N_i(0,1))
   ‚îÇ       ‚îÇ   x'_filho ‚Üê x_filho + N(0, (œÉ'_filho)¬≤I)
   ‚îÇ       ‚îî‚îÄ‚îÄ Avaliar f(x'_filho)
   ‚îÇ
   ‚îú‚îÄ‚îÄ üéØ SELE√á√ÉO (Œº+Œª)
   ‚îÇ   ‚îú‚îÄ‚îÄ Combinar pais P e filhos C
   ‚îÇ   ‚îú‚îÄ‚îÄ Ordenar por fitness
   ‚îÇ   ‚îî‚îÄ‚îÄ Selecionar melhores Œº para pr√≥xima gera√ß√£o
   ‚îÇ
   ‚îî‚îÄ‚îÄ Incrementar gera√ß√£o

üèÜ 3. RETORNAR melhor indiv√≠duo
```

### **2.3 (Œº,Œª)-ES: Estrat√©gia N√£o-Geracional**

Diferen√ßa principal: apenas filhos competem (Œª ‚â• Œº obrigat√≥rio)

```
üéØ SELE√á√ÉO (Œº,Œª)
   ‚îú‚îÄ‚îÄ Avaliar apenas os Œª filhos
   ‚îú‚îÄ‚îÄ Ordenar filhos por fitness
   ‚îú‚îÄ‚îÄ Selecionar melhores Œº filhos
   ‚îî‚îÄ‚îÄ Pais s√£o descartados
```

**Vantagens de (Œº,Œª):**
- ‚ùå N√£o-elitista: perde melhor solu√ß√£o temporariamente
- ‚úÖ Melhor em ambientes ruidosos
- ‚úÖ Evita converg√™ncia prematura
- ‚úÖ Permite mudan√ßas no landscape

---

## **3. üîß Auto-adapta√ß√£o de Par√¢metros**

### **3.1 Auto-adapta√ß√£o de Step-sizes**

Um dos recursos mais poderosos das ES √© a auto-adapta√ß√£o:

```python
# Auto-adapta√ß√£o de um √∫nico œÉ (isotropic)
œÉ' = œÉ * exp(œÑ * N(0, 1))
x' = x + N(0, (œÉ')¬≤¬∑I)

# Par√¢metro de aprendizado t√≠pico
œÑ = 1 / sqrt(2¬∑n)  # n = dimensionalidade
```

**Variantes:**

#### **1. Isotropic (1 step-size para todas as dimens√µes)**
```python
œÉ' = œÉ * exp(œÑ ¬∑ N(0,1))
```
- ‚úÖ Simples
- ‚ùå N√£o captura diferentes escalas

#### **2. Individual Step-sizes (œÉ por dimens√£o)**
```python
œÉ'_i = œÉ_i * exp(œÑ' ¬∑ N(0,1) + œÑ ¬∑ N_i(0,1))
x'_i = x_i + œÉ'_i ¬∑ N_i(0,1)

œÑ' = 1 / sqrt(2¬∑n)    # Aprendizado global
œÑ = 1 / sqrt(2¬∑sqrt(n))  # Aprendizado local
```
- ‚úÖ Captura escalas diferentes
- ‚ö™ Mais par√¢metros

#### **3. Correlated Mutations (com rota√ß√µes)**
```python
# Inclui correla√ß√µes entre dimens√µes
# Usa matriz de covari√¢ncia completa
# Complexo, mas mais poderoso
```

### **3.2 CMA-ES (Covariance Matrix Adaptation)**

A variante mais avan√ßada, considerada estado-da-arte:

**Ideia:** Adaptar uma matriz de covari√¢ncia completa para capturar:
- Escalas diferentes nas dimens√µes
- Correla√ß√µes/rota√ß√µes no espa√ßo de busca
- Dire√ß√£o de busca promissora

```
C ‚Üê matriz de covari√¢ncia (n√ón)
œÉ ‚Üê step-size global
m ‚Üê m√©dia da distribui√ß√£o (centr√≥ide)

Em cada gera√ß√£o:
1. Gerar Œª amostras: x_i ~ N(m, œÉ¬≤C)
2. Selecionar Œº melhores
3. Atualizar m (m√©dia dos melhores)
4. Atualizar C (dire√ß√µes promissoras)
5. Atualizar œÉ (controle de converg√™ncia)
```

---

## **4. üíª Implementa√ß√£o em Python**

### **4.1 (1+1)-ES B√°sica**

```python
import numpy as np

class OnePlusOneES:
    """
    Implementa√ß√£o de (1+1)-ES com regra 1/5
    """
    
    def __init__(self, objective_function, bounds, 
                 sigma_init=1.0, max_iter=1000):
        """
        Args:
            objective_function: Fun√ß√£o a minimizar
            bounds: Lista de (min, max) para cada dimens√£o
            sigma_init: Step-size inicial
            max_iter: N√∫mero m√°ximo de gera√ß√µes
        """
        self.f = objective_function
        self.bounds = np.array(bounds)
        self.dim = len(bounds)
        self.sigma = sigma_init
        self.max_iter = max_iter
        
        # Para regra 1/5
        self.n_window = int(self.dim * 10)  # Janela de observa√ß√£o
        self.success_history = []
        self.c = 0.82  # Fator de ajuste
    
    def initialize(self):
        """Inicializa solu√ß√£o aleat√≥ria"""
        x = np.random.rand(self.dim)
        for i in range(self.dim):
            x[i] = self.bounds[i, 0] + x[i] * (
                self.bounds[i, 1] - self.bounds[i, 0]
            )
        return x
    
    def mutate(self, x):
        """Aplica muta√ß√£o gaussiana"""
        mutation = np.random.normal(0, self.sigma, self.dim)
        x_new = x + mutation
        # Garantir limites
        x_new = np.clip(x_new, self.bounds[:, 0], self.bounds[:, 1])
        return x_new
    
    def adapt_stepsize(self):
        """Adapta step-size usando regra 1/5"""
        if len(self.success_history) >= self.n_window:
            # Calcular taxa de sucesso
            success_rate = np.mean(self.success_history[-self.n_window:])
            
            if success_rate > 0.2:  # 1/5 = 0.2
                self.sigma = self.sigma / self.c  # Aumentar
            elif success_rate < 0.2:
                self.sigma = self.sigma * self.c  # Diminuir
    
    def optimize(self):
        """Executa otimiza√ß√£o"""
        # Inicializa√ß√£o
        x = self.initialize()
        fitness = self.f(x)
        best_x = x.copy()
        best_fitness = fitness
        
        history = {
            'best_fitness': [best_fitness],
            'sigma': [self.sigma]
        }
        
        # Loop evolutivo
        for generation in range(self.max_iter):
            # Muta√ß√£o
            x_new = self.mutate(x)
            fitness_new = self.f(x_new)
            
            # Sele√ß√£o
            if fitness_new <= fitness:
                x = x_new
                fitness = fitness_new
                self.success_history.append(1)
                
                if fitness < best_fitness:
                    best_x = x.copy()
                    best_fitness = fitness
            else:
                self.success_history.append(0)
            
            # Adaptar step-size
            self.adapt_stepsize()
            
            # Registrar hist√≥rico
            history['best_fitness'].append(best_fitness)
            history['sigma'].append(self.sigma)
        
        return best_x, best_fitness, history

# Exemplo de uso
def sphere(x):
    return np.sum(x**2)

bounds = [(-5, 5)] * 10
es = OnePlusOneES(sphere, bounds, sigma_init=1.0, max_iter=500)
solution, fitness, history = es.optimize()

print(f"Solu√ß√£o: {solution}")
print(f"Fitness: {fitness:.8f}")
print(f"Sigma final: {history['sigma'][-1]:.6f}")
```

### **4.2 (Œº+Œª)-ES com Auto-adapta√ß√£o**

```python
class MuPlusLambdaES:
    """
    (Œº+Œª)-ES com auto-adapta√ß√£o individual de step-sizes
    """
    
    def __init__(self, objective_function, bounds, 
                 mu=15, lambda_=100, rho=None, max_iter=500):
        """
        Args:
            mu: N√∫mero de pais
            lambda_: N√∫mero de filhos (deve ser >= mu)
            rho: N√∫mero de pais para recombina√ß√£o (None = mu)
        """
        self.f = objective_function
        self.bounds = np.array(bounds)
        self.dim = len(bounds)
        self.mu = mu
        self.lambda_ = lambda_
        self.rho = rho or mu
        self.max_iter = max_iter
        
        # Par√¢metros de auto-adapta√ß√£o
        self.tau = 1.0 / np.sqrt(2 * self.dim)
        self.tau_prime = 1.0 / np.sqrt(2 * np.sqrt(self.dim))
    
    def initialize_population(self):
        """Inicializa popula√ß√£o com (x, œÉ)"""
        population = []
        for _ in range(self.mu):
            # Solu√ß√£o
            x = np.random.rand(self.dim)
            for i in range(self.dim):
                x[i] = self.bounds[i, 0] + x[i] * (
                    self.bounds[i, 1] - self.bounds[i, 0]
                )
            
            # Step-sizes individuais
            sigma = np.ones(self.dim) * 0.5
            
            population.append({
                'x': x,
                'sigma': sigma,
                'fitness': self.f(x)
            })
        
        return population
    
    def recombination_intermediate(self, parents, key):
        """Recombina√ß√£o intermedi√°ria (m√©dia)"""
        return np.mean([p[key] for p in parents], axis=0)
    
    def mutate(self, parent):
        """Muta√ß√£o com auto-adapta√ß√£o"""
        # Auto-adaptar step-sizes
        global_factor = self.tau_prime * np.random.normal()
        individual_factors = self.tau * np.random.normal(size=self.dim)
        
        sigma_new = parent['sigma'] * np.exp(
            global_factor + individual_factors
        )
        
        # Garantir œÉ m√≠nimo
        sigma_new = np.maximum(sigma_new, 1e-10)
        
        # Mutar solu√ß√£o
        x_new = parent['x'] + sigma_new * np.random.normal(size=self.dim)
        
        # Garantir limites
        x_new = np.clip(x_new, self.bounds[:, 0], self.bounds[:, 1])
        
        return {
            'x': x_new,
            'sigma': sigma_new,
            'fitness': self.f(x_new)
        }
    
    def select_parents(self, population, n):
        """Seleciona n pais aleatoriamente"""
        return [population[i] for i in np.random.choice(
            len(population), n, replace=False
        )]
    
    def optimize(self):
        """Executa (Œº+Œª)-ES"""
        # Inicializa√ß√£o
        population = self.initialize_population()
        
        # Melhor solu√ß√£o
        population.sort(key=lambda ind: ind['fitness'])
        best = population[0].copy()
        
        history = {
            'best_fitness': [best['fitness']],
            'avg_fitness': [np.mean([ind['fitness'] for ind in population])],
            'avg_sigma': [np.mean([np.mean(ind['sigma']) for ind in population])]
        }
        
        # Loop evolutivo
        for generation in range(self.max_iter):
            offspring = []
            
            # Gerar Œª filhos
            for _ in range(self.lambda_):
                # Selecionar œÅ pais
                parents = self.select_parents(population, self.rho)
                
                # Recombina√ß√£o
                child = {
                    'x': self.recombination_intermediate(parents, 'x'),
                    'sigma': self.recombination_intermediate(parents, 'sigma')
                }
                
                # Muta√ß√£o
                child = self.mutate(child)
                offspring.append(child)
            
            # Sele√ß√£o (Œº+Œª): combinar pais e filhos
            combined = population + offspring
            combined.sort(key=lambda ind: ind['fitness'])
            
            # Selecionar melhores Œº
            population = combined[:self.mu]
            
            # Atualizar melhor
            if population[0]['fitness'] < best['fitness']:
                best = population[0].copy()
            
            # Registrar hist√≥rico
            history['best_fitness'].append(best['fitness'])
            history['avg_fitness'].append(
                np.mean([ind['fitness'] for ind in population])
            )
            history['avg_sigma'].append(
                np.mean([np.mean(ind['sigma']) for ind in population])
            )
        
        return best['x'], best['fitness'], history

# Exemplo de uso
def rastrigin(x):
    n = len(x)
    return 10*n + np.sum(x**2 - 10*np.cos(2*np.pi*x))

bounds = [(-5.12, 5.12)] * 10
es = MuPlusLambdaES(
    rastrigin, 
    bounds, 
    mu=15, 
    lambda_=100, 
    rho=7,
    max_iter=200
)

solution, fitness, history = es.optimize()
print(f"Melhor fitness: {fitness:.6f}")
```

### **4.3 CMA-ES Simplificada**

```python
class SimpleCMAES:
    """
    Implementa√ß√£o simplificada de CMA-ES
    Para produ√ß√£o, use biblioteca pycma
    """
    
    def __init__(self, objective_function, bounds, 
                 pop_size=None, sigma_init=0.5, max_iter=500):
        self.f = objective_function
        self.bounds = np.array(bounds)
        self.dim = len(bounds)
        self.sigma = sigma_init
        self.max_iter = max_iter
        
        # Tamanho da popula√ß√£o
        self.lambda_ = pop_size or (4 + int(3 * np.log(self.dim)))
        self.mu = self.lambda_ // 2
        
        # Pesos para recombina√ß√£o
        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))
        self.weights = self.weights / np.sum(self.weights)
        
        # M√©dia inicial
        self.mean = np.random.rand(self.dim)
        for i in range(self.dim):
            self.mean[i] = self.bounds[i, 0] + self.mean[i] * (
                self.bounds[i, 1] - self.bounds[i, 0]
            )
        
        # Matriz de covari√¢ncia
        self.C = np.eye(self.dim)
        
        # Caminhos de evolu√ß√£o
        self.pc = np.zeros(self.dim)
        self.ps = np.zeros(self.dim)
        
        # Par√¢metros de aprendizado
        self.cc = 4 / (self.dim + 4)
        self.cs = 4 / (self.dim + 4)
        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mu)
        self.cmu = min(1 - self.c1, 2 * (self.mu - 2 + 1/self.mu) / 
                      ((self.dim + 2)**2 + self.mu))
        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1) + self.cs
    
    def optimize(self):
        """Executa CMA-ES"""
        history = {'best_fitness': []}
        best_fitness = np.inf
        best_solution = None
        
        for generation in range(self.max_iter):
            # Gerar popula√ß√£o
            population = []
            for _ in range(self.lambda_):
                # Amostra da distribui√ß√£o normal multivariada
                z = np.random.normal(0, 1, self.dim)
                y = np.dot(np.linalg.cholesky(self.C), z)
                x = self.mean + self.sigma * y
                
                # Garantir limites
                x = np.clip(x, self.bounds[:, 0], self.bounds[:, 1])
                
                fitness = self.f(x)
                population.append((fitness, x, y))
            
            # Ordenar por fitness
            population.sort(key=lambda item: item[0])
            
            # Atualizar melhor
            if population[0][0] < best_fitness:
                best_fitness = population[0][0]
                best_solution = population[0][1].copy()
            
            history['best_fitness'].append(best_fitness)
            
            # Selecionar Œº melhores
            selected = population[:self.mu]
            
            # Atualizar m√©dia (recombina√ß√£o ponderada)
            old_mean = self.mean.copy()
            self.mean = np.sum([
                self.weights[i] * selected[i][1] 
                for i in range(self.mu)
            ], axis=0)
            
            # Atualizar caminhos de evolu√ß√£o e covari√¢ncia
            # (simplificado - vers√£o completa √© mais complexa)
            
        return best_solution, best_fitness, history

# Para uso em produ√ß√£o, recomenda-se:
# pip install cma
# import cma
# es = cma.CMAEvolutionStrategy(x0, sigma0)
# es.optimize(objective_function)
```

---

## **5. üéØ Exemplos de Aplica√ß√£o**

### **5.1 Controle de Rob√¥**

```python
def robot_control_optimization():
    """
    Otimiza par√¢metros de controlador de rob√¥
    """
    def simulate_robot(params):
        """
        Simula rob√¥ com par√¢metros de controle
        Retorna erro acumulado (a minimizar)
        """
        # params = [kp, ki, kd, ...] (par√¢metros PID, etc.)
        
        # Simula√ß√£o simplificada
        error_total = 0
        state = 0
        target = 10
        
        for t in range(100):
            error = target - state
            control = params[0] * error  # PID simplificado
            state += control * 0.1
            error_total += abs(error)
        
        return error_total
    
    # Otimizar com ES
    bounds = [(0, 10)] * 3  # kp, ki, kd
    es = MuPlusLambdaES(
        simulate_robot,
        bounds,
        mu=10,
        lambda_=70,
        max_iter=100
    )
    
    best_params, best_error, history = es.optimize()
    
    print(f"Melhores par√¢metros: {best_params}")
    print(f"Erro final: {best_error:.4f}")
    
    return best_params

# Executar
robot_params = robot_control_optimization()
```

### **5.2 Evolution Strategies para Reinforcement Learning**

```python
def es_for_rl():
    """
    Usa ES para treinar pol√≠tica de RL
    Baseado em "Evolution Strategies as a Scalable Alternative to RL"
    (Salimans et al., OpenAI, 2017)
    """
    class NeuralPolicy:
        """Pol√≠tica neural simples"""
        def __init__(self, input_dim, hidden_dim, output_dim):
            self.input_dim = input_dim
            self.hidden_dim = hidden_dim
            self.output_dim = output_dim
            
            # N√∫mero total de par√¢metros
            self.n_params = (input_dim * hidden_dim + hidden_dim +
                           hidden_dim * output_dim + output_dim)
        
        def set_params(self, params):
            """Define pesos da rede"""
            self.params = params
        
        def forward(self, state):
            """Forward pass simplificado"""
            # Implementa√ß√£o simplificada
            # Na pr√°tica, seria uma rede neural completa
            idx = 0
            # W1: input_dim x hidden_dim
            W1_size = self.input_dim * self.hidden_dim
            W1 = params[idx:idx+W1_size].reshape(self.hidden_dim, self.input_dim)
            idx += W1_size
            # ... resto dos par√¢metros
            
            # Computar a√ß√£o
            action = np.tanh(W1 @ state)  # Simplificado
            return action
    
    def evaluate_policy(params, n_episodes=5):
        """
        Avalia pol√≠tica em ambiente
        Retorna recompensa total (negativa para minimiza√ß√£o)
        """
        policy = NeuralPolicy(input_dim=4, hidden_dim=8, output_dim=2)
        policy.set_params(params)
        
        total_reward = 0
        for episode in range(n_episodes):
            # Simula√ß√£o de epis√≥dio
            state = np.random.randn(4)
            episode_reward = 0
            
            for step in range(100):
                action = policy.forward(state)
                # Simular ambiente
                reward = -np.sum(state**2)  # Exemplo simplificado
                episode_reward += reward
                # Atualizar estado (simplificado)
                state = state + 0.1 * action[:len(state)]
            
            total_reward += episode_reward
        
        return -total_reward / n_episodes  # Negativo para minimiza√ß√£o
    
    # Configurar ES
    policy = NeuralPolicy(input_dim=4, hidden_dim=8, output_dim=2)
    n_params = policy.n_params
    
    # Bounds amplos para pesos neurais
    bounds = [(-2, 2)] * n_params
    
    es = MuPlusLambdaES(
        evaluate_policy,
        bounds,
        mu=20,
        lambda_=100,
        max_iter=50
    )
    
    best_params, best_reward, history = es.optimize()
    
    print(f"Melhor recompensa: {-best_reward:.2f}")
    
    return best_params

# Nota: Para uso real em RL, veja:
# - OpenAI ES: https://github.com/openai/evolution-strategies-starter
# - PyTorch ES: estorch
```

---

## **6. ‚öôÔ∏è Configura√ß√£o e Diretrizes**

### **6.1 Escolha de Œº e Œª**

| Estrat√©gia | Œº | Œª | Rela√ß√£o | Uso |
|-----------|---|---|---------|-----|
| **(1+1)-ES** | 1 | 1 | - | Problemas simples, r√°pido |
| **(1+Œª)-ES** | 1 | 10-100 | Œª >> 1 | Explora√ß√£o ampla |
| **(Œº+Œª)-ES** | 10-30% de Œª | ~5Œº | Œª ‚â• Œº | Balanceado, elitista |
| **(Œº,Œª)-ES** | ~1/7 de Œª | 7Œº | Œª ‚â• 7Œº | Ambientes ruidosos |
| **CMA-ES** | Œª/2 | 4+‚åä3ln(n)‚åã | Œº = Œª/2 | Estado-da-arte |

**Regras Gerais:**
- **Œº pequeno:** Converg√™ncia r√°pida, menos diversidade
- **Œº grande:** Mais robusto, converg√™ncia lenta
- **Œª grande:** Mais explora√ß√£o, mais avalia√ß√µes
- **Raz√£o Œª/Œº:** Tipicamente 5-7

### **6.2 Step-size Inicial (œÉ)**

```python
# Regras de bolso
œÉ_init = (bounds_max - bounds_min) / 3  # Cobre ~99% da faixa
œÉ_init = 0.3  # Para problema normalizado [0,1]
œÉ_init = 1.0  # Para problema centrado em 0

# Ajuste adaptativo
# ES ajusta automaticamente, ent√£o valor inicial n√£o √© cr√≠tico
```

### **6.3 Quando Usar Cada Variante**

#### **(1+1)-ES**
```
‚úÖ Usar quando:
- Problema simples/unimodal
- Recursos limitados
- Prototipagem r√°pida
- Baseline para compara√ß√£o
```

#### **(Œº+Œª)-ES**
```
‚úÖ Usar quando:
- Precisa de elitismo
- Converg√™ncia est√°vel importante
- Ambiente determin√≠stico
- Problemas multimodais
```

#### **(Œº,Œª)-ES**
```
‚úÖ Usar quando:
- Fun√ß√£o objetivo ruidosa
- Landscape din√¢mico
- Evitar converg√™ncia prematura
- Œª >> Œº dispon√≠vel
```

#### **CMA-ES**
```
‚úÖ Usar quando:
- Problema complexo/multimodal
- Alta dimensionalidade
- Precisa do melhor desempenho
- Recursos computacionais dispon√≠veis
- Fun√ß√£o tem correla√ß√µes entre vari√°veis
```

---

## **7. ‚úÖ Vantagens e ‚ùå Desvantagens**

### **7.1 ‚úÖ Vantagens**

| Vantagem | Descri√ß√£o | Benef√≠cio |
|----------|-----------|-----------|
| **Auto-adapta√ß√£o** | Par√¢metros evoluem automaticamente | Menos ajuste manual |
| **Robustez a Ru√≠do** | (Œº,Œª) lida bem com avalia√ß√µes ruidosas | Funciona em ambientes reais |
| **Sem Gradiente** | N√£o precisa de derivadas | Fun√ß√µes black-box |
| **Espa√ßo Cont√≠nuo** | Projetado para valores reais | Ideal para otimiza√ß√£o num√©rica |
| **Teoria S√≥lida** | Base matem√°tica forte | Entendimento profundo |
| **Paraleliz√°vel** | Avalia√ß√µes independentes | Escal√°vel |
| **CMA-ES** | Estado-da-arte em otimiza√ß√£o | Melhor performance |

### **7.2 ‚ùå Desvantagens**

| Desvantagem | Descri√ß√£o | Mitiga√ß√£o |
|-------------|-----------|-----------|
| **Custo Computacional** | Muitas avalia√ß√µes necess√°rias | Paralelizar, usar surrogates |
| **Apenas Cont√≠nuo** | N√£o funciona em discreto | Usar GA para discreto |
| **Converg√™ncia Lenta** | Pode ser lento | Usar CMA-ES ou hibridizar |
| **Dimensionalidade** | CMA-ES escala O(n¬≤) em mem√≥ria | Usar variantes para alta dimens√£o |
| **Complexidade** | CMA-ES √© complexo | Usar bibliotecas prontas |

### **7.3 üéØ Quando Usar ES**

#### **‚úÖ Cen√°rios Ideais:**
- ‚úÖ Otimiza√ß√£o num√©rica cont√≠nua
- ‚úÖ Fun√ß√µes ruidosas ou estoc√°sticas
- ‚úÖ N√£o h√° gradiente dispon√≠vel
- ‚úÖ Problemas multimodais
- ‚úÖ Reinforcement Learning (treinar pol√≠ticas)
- ‚úÖ Controle e rob√≥tica
- ‚úÖ Engenharia (design, calibra√ß√£o)
- ‚úÖ Alta dimensionalidade (CMA-ES at√© ~100D)

#### **‚ùå Evite ES quando:**
- ‚ùå Fun√ß√£o √© convexa e suave (usar otimiza√ß√£o baseada em gradiente)
- ‚ùå Problema √© discreto (usar GA)
- ‚ùå Avalia√ß√£o √© extremamente cara
- ‚ùå Dimensionalidade √© muito alta (> 1000D)
- ‚ùå Precisa de garantias te√≥ricas

---

## **8. üî¨ Variantes Avan√ßadas**

### **8.1 Natural Evolution Strategies (NES)**

```python
"""
NES usa gradiente natural da distribui√ß√£o de busca
Mais eficiente que ES padr√£o em alguns casos
"""

class NaturalES:
    """
    Implementa√ß√£o simplificada de NES
    """
    def __init__(self, objective_function, dim, pop_size=50):
        self.f = objective_function
        self.dim = dim
        self.pop_size = pop_size
        
        # Par√¢metros da distribui√ß√£o
        self.mu = np.zeros(dim)
        self.sigma = 1.0
        
        # Learning rates
        self.lr_mu = 0.1
        self.lr_sigma = 0.05
    
    def optimize(self, max_iter=500):
        history = {'best_fitness': []}
        
        for generation in range(max_iter):
            # Gerar popula√ß√£o
            noise = np.random.randn(self.pop_size, self.dim)
            population = self.mu + self.sigma * noise
            
            # Avaliar
            fitness = np.array([self.f(ind) for ind in population])
            
            # Normalizar fitness (utilities)
            utilities = compute_centered_ranks(fitness)
            
            # Gradiente natural
            grad_mu = (1.0 / (self.pop_size * self.sigma)) * np.dot(
                noise.T, utilities
            )
            grad_sigma = (1.0 / (self.pop_size * self.sigma)) * np.dot(
                (noise**2 - 1).T, utilities
            ).mean()
            
            # Atualizar par√¢metros
            self.mu += self.lr_mu * grad_mu
            self.sigma *= np.exp(0.5 * self.lr_sigma * grad_sigma)
            
            history['best_fitness'].append(np.min(fitness))
        
        return self.mu, history

def compute_centered_ranks(fitness):
    """Calcula utilities baseados em ranking"""
    ranks = np.argsort(np.argsort(fitness))
    utilities = ranks / (len(ranks) - 1) - 0.5
    return utilities
```

### **8.2 Separable CMA-ES (Sep-CMA-ES)**

Para alta dimensionalidade, usa apenas diagonal da covari√¢ncia:

```python
"""
Sep-CMA-ES: Matriz diagonal ao inv√©s de completa
Reduz complexidade de O(n¬≤) para O(n)
Funciona bem em fun√ß√µes separ√°veis
"""

class SepCMAES:
    """CMA-ES com covari√¢ncia diagonal"""
    def __init__(self, objective_function, dim, pop_size=None):
        self.f = objective_function
        self.dim = dim
        self.lambda_ = pop_size or (4 + int(3 * np.log(dim)))
        
        # Apenas diagonal
        self.sigma = np.ones(dim)  # Step-sizes por dimens√£o
        self.mean = np.zeros(dim)
        
    # Implementa√ß√£o similar a CMA-ES mas com diagonal
```

### **8.3 OpenAI Evolution Strategies**

Vers√£o escal√°vel para deep learning:

```python
"""
OpenAI ES: Paraleliza√ß√£o massiva
Usa "virtual batch normalization" via seeds
Escala para milhares de cores
"""

class OpenAIES:
    """
    Vers√£o simplificada do OpenAI ES
    """
    def __init__(self, objective_function, dim, 
                 pop_size=1000, learning_rate=0.01):
        self.f = objective_function
        self.dim = dim
        self.pop_size = pop_size
        self.lr = learning_rate
        self.theta = np.zeros(dim)  # Par√¢metros
        self.sigma = 0.1
    
    def optimize_parallel(self, max_iter=100):
        """
        Otimiza√ß√£o paralela
        Na pr√°tica, usar multiprocessing ou distributed computing
        """
        for generation in range(max_iter):
            # Gerar seeds
            seeds = np.random.randint(0, 2**32, self.pop_size)
            
            # Avaliar em paralelo (simplificado aqui)
            rewards = []
            for seed in seeds:
                np.random.seed(seed)
                epsilon = np.random.randn(self.dim)
                reward = self.f(self.theta + self.sigma * epsilon)
                rewards.append((reward, epsilon, seed))
            
            # Atualizar usando gradient estimator
            gradient = np.zeros(self.dim)
            for reward, epsilon, seed in rewards:
                gradient += reward * epsilon
            
            gradient /= (self.pop_size * self.sigma)
            
            # Gradient ascent (assumindo maximiza√ß√£o)
            self.theta += self.lr * gradient
        
        return self.theta

# Para uso real:
# - Usar Ray ou MPI para paraleliza√ß√£o
# - Ver: https://github.com/openai/evolution-strategies-starter
```

---

## **9. üìö Aplica√ß√µes Pr√°ticas**

### **9.1 Design de Aeronaves**

```python
def aircraft_design_optimization():
    """
    Otimiza par√¢metros de design de aeronave
    """
    def evaluate_aircraft(params):
        """
        params: [wingspan, chord, thickness, sweep, ...]
        Retorna: custo (peso + drag + constraints)
        """
        # Simula√ß√£o CFD ou modelo anal√≠tico
        weight = params[0] * params[1] * 10  # Simplificado
        drag = params[2]**2 + params[3]**2
        
        # Restri√ß√µes (penalidades)
        penalty = 0
        if params[0] < 10:  # wingspan m√≠nimo
            penalty += 1000
        
        return weight + drag + penalty
    
    # Bounds: [wingspan, chord, thickness, sweep]
    bounds = [(10, 30), (1, 5), (0.1, 0.5), (0, 45)]
    
    es = MuPlusLambdaES(
        evaluate_aircraft,
        bounds,
        mu=20,
        lambda_=140,
        max_iter=200
    )
    
    best_design, cost, history = es.optimize()
    return best_design, cost
```

### **9.2 Calibra√ß√£o de Modelos Clim√°ticos**

```python
def climate_model_calibration():
    """
    Calibra par√¢metros de modelo clim√°tico
    """
    def climate_model_error(params):
        """
        Compara simula√ß√£o com dados observados
        params: par√¢metros f√≠sicos do modelo
        """
        # Executar modelo clim√°tico
        simulated_temp = run_climate_model(params)  # Fun√ß√£o externa
        
        # Comparar com observa√ß√µes
        observed_temp = load_observations()
        
        # Erro RMSE
        error = np.sqrt(np.mean((simulated_temp - observed_temp)**2))
        
        return error
    
    # Par√¢metros: [albedo, cloud_feedback, ocean_heat, ...]
    bounds = [(0.2, 0.4), (-2, 2), (0, 100), ...]
    
    # Usar (Œº,Œª)-ES por ser robusto a ru√≠do
    es = MuCommaLambdaES(  # Implementar variante comma
        climate_model_error,
        bounds,
        mu=30,
        lambda_=210,
        max_iter=100
    )
    
    best_params, error, history = es.optimize()
    return best_params
```

---

## **10. üîó Refer√™ncias e Recursos**

### **10.1 üìö Publica√ß√µes Fundamentais**

1. **Rechenberg, I. (1965).** *"Cybernetic Solution Path of an Experimental Problem"*. 
   - üåü Trabalho original que introduziu Evolution Strategies
   - Royal Aircraft Establishment, Library Translation

2. **Schwefel, H. P. (1981).** *"Numerical Optimization of Computer Models"*. 
   - üìñ Primeira formula√ß√£o completa de ES
   - John Wiley & Sons

3. **Hansen, N., & Ostermeier, A. (2001).** *"Completely Derandomized Self-Adaptation in Evolution Strategies"*. Evolutionary Computation, 9(2), 159-195.
   - üéØ Artigo definitivo sobre CMA-ES
   - Estado-da-arte em otimiza√ß√£o

4. **Beyer, H. G., & Schwefel, H. P. (2002).** *"Evolution Strategies ‚Äì A Comprehensive Introduction"*. Natural Computing, 1(1), 3-52.
   - üìä Survey completo sobre ES
   - Teoria e pr√°tica

5. **Salimans, T., et al. (2017).** *"Evolution Strategies as a Scalable Alternative to Reinforcement Learning"*. arXiv:1703.03864.
   - üöÄ OpenAI ES para deep RL
   - Paraleliza√ß√£o massiva

### **10.2 üìñ Livros Recomendados**

- **"Introduction to Evolutionary Computing"** - Eiben & Smith (2015)
  - Cap√≠tulo completo sobre ES
  
- **"Evolutionary Algorithms in Theory and Practice"** - B√§ck (1996)
  - Foco em ES e an√°lise te√≥rica

- **"Natural Computing Series: Theory of Evolutionary Algorithms"** - Beyer (2001)
  - An√°lise matem√°tica profunda

### **10.3 üõ†Ô∏è Bibliotecas e Ferramentas**

#### **Python**
```python
# 1. pycma - CMA-ES de refer√™ncia
pip install cma
import cma
es = cma.CMAEvolutionStrategy(10 * [0], 0.5)
es.optimize(objective_function)

# 2. deap - Framework completo
pip install deap
from deap import algorithms, base, tools

# 3. evotorch - ES moderno com PyTorch
pip install evotorch

# 4. nevergrad - Facebook Research
pip install nevergrad
import nevergrad as ng
optimizer = ng.optimizers.CMA(parametrization=10)

# 5. estorch - ES para PyTorch/RL
pip install estorch
```

#### **Outras Linguagens**
- **Java:** JCLEC, ECJ
- **C++:** Shark ML Library
- **MATLAB:** Global Optimization Toolbox
- **R:** cmaes package

### **10.4 üåê Recursos Online**

| Recurso | Descri√ß√£o | Link |
|---------|-----------|------|
| **CMA-ES Tutorial** | Tutorial oficial de Hansen | cma.gforge.inria.fr |
| **OpenAI ES GitHub** | Implementa√ß√£o escal√°vel | github.com/openai/evolution-strategies-starter |
| **Nikolaus Hansen's Page** | Papers, c√≥digo, tutoriais | www.cmap.polytechnique.fr/~nikolaus.hansen |
| **Evolution Strategies Wiki** | Enciclop√©dia de ES | www.scholarpedia.org/article/Evolution_strategies |

### **10.5 üéì Cursos e Tutoriais**

- **Coursera:** Evolutionary Algorithms
- **MIT OCW:** Computational Evolutionary Biology
- **YouTube:** Lectures by Nikolaus Hansen (CMA-ES)
- **Tutorial Papers:** Hansen (2016) "The CMA Evolution Strategy: A Tutorial"

---

## **11. üéØ Conclus√£o**

As Estrat√©gias de Evolu√ß√£o representam uma das abordagens mais **sofisticadas e eficazes** para otimiza√ß√£o cont√≠nua. Suas caracter√≠sticas principais s√£o:

### **üîë Principais Aprendizados**

1. **Auto-adapta√ß√£o:** O conceito revolucion√°rio de evoluir a estrat√©gia de busca junto com a solu√ß√£o
2. **Robustez:** Especialmente em ambientes ruidosos e din√¢micos
3. **Teoria S√≥lida:** Base matem√°tica forte permite entendimento profundo
4. **CMA-ES:** Estado-da-arte em otimiza√ß√£o cont√≠nua
5. **Escalabilidade:** OpenAI ES demonstrou aplicabilidade em deep learning

### **üí° Compara√ß√£o com Outros M√©todos**

| M√©todo | Gradiente | Multimodal | Ru√≠do | Dimens√£o Alta | Auto-adapta√ß√£o |
|--------|-----------|------------|-------|---------------|----------------|
| **ES** | ‚ùå | ‚úÖ | ‚úÖ‚úÖ | ‚úÖ | ‚úÖ‚úÖ |
| **GA** | ‚ùå | ‚úÖ | ‚ö™ | ‚ö™ | ‚ùå |
| **DE** | ‚ùå | ‚úÖ‚úÖ | ‚ö™ | ‚úÖ | ‚ö™ |
| **Gradient Descent** | ‚úÖ | ‚ùå | ‚ùå | ‚úÖ‚úÖ | ‚ùå |
| **Particle Swarm** | ‚ùå | ‚úÖ | ‚ö™ | ‚ö™ | ‚ùå |

### **üöÄ Pr√≥ximos Passos**

1. **Comece Simples:** Implemente (1+1)-ES
2. **Entenda Auto-adapta√ß√£o:** Experimente com diferentes œÑ
3. **Use CMA-ES:** Para problemas reais, use biblioteca `pycma`
4. **Explore Paraleliza√ß√£o:** Teste OpenAI ES para problemas grandes
5. **Compare:** Benchmark contra DE e GA
6. **Aplique:** Use em seus problemas de otimiza√ß√£o

### **üåü Reflex√£o Final**

As Estrat√©gias de Evolu√ß√£o demonstram um princ√≠pio profundo: **n√£o apenas a solu√ß√£o deve evoluir, mas tamb√©m a estrat√©gia para encontr√°-la**. Esta meta-evolu√ß√£o permite que o algoritmo se adapte automaticamente √†s caracter√≠sticas do problema, tornando-o excepcionalmente robusto e vers√°til.

> *"A ess√™ncia das Estrat√©gias de Evolu√ß√£o est√° em reconhecer que o caminho para a solu√ß√£o √© t√£o importante quanto a solu√ß√£o em si - e ambos devem evoluir juntos."*

**Em destaque: CMA-ES** √© considerado por muitos como o melhor algoritmo de otimiza√ß√£o livre de gradiente para problemas cont√≠nuos de at√© ~100 dimens√µes. Se voc√™ precisa otimizar uma fun√ß√£o black-box, CMA-ES deve ser sua primeira escolha.

---

**üîó Continue explorando:**
- üìñ Compare com [**Differential Evolution**](differential_evolution.md) para entender diferen√ßas
- üß¨ Veja [**Genetic Algorithms**](genetic_algorithms.md) para abordagem discreta
- üéØ Explore [**Algoritmos Evolucion√°rios**](README.md) para vis√£o geral
- üå≥ Descubra [**Genetic Programming**](genetic_programming.md) para evolu√ß√£o de programas

**Voltar para:** [Documenta√ß√£o de Algoritmos](../README.md) | [Documenta√ß√£o Principal](../../README.md)
