# M√©todos Num√©ricos - M√≠nimos Quadrados (MMQ) üî¢

O **M√©todo dos M√≠nimos Quadrados** (MMQ, ou Least Squares em ingl√™s) √© uma t√©cnica fundamental de otimiza√ß√£o para ajustar modelos matem√°ticos a dados observados. √â a base da regress√£o linear e muitos outros m√©todos estat√≠sticos e de aprendizado de m√°quina.

---

## **1. üéØ Fundamentos Te√≥ricos**

### **1.1 O Problema**

Dado um conjunto de observa√ß√µes (x·µ¢, y·µ¢), queremos encontrar par√¢metros Œ≤ que minimizem o **erro quadr√°tico** entre valores observados e preditos.

**Formula√ß√£o:**
```
Minimizar: S(Œ≤) = Œ£(y·µ¢ - f(x·µ¢, Œ≤))¬≤

onde:
‚Ä¢ y·µ¢: valores observados
‚Ä¢ f(x·µ¢, Œ≤): valores preditos pelo modelo
‚Ä¢ e·µ¢ = y·µ¢ - f(x·µ¢, Œ≤): res√≠duos
```

### **1.2 Por Que Quadrados?**

**Raz√µes Hist√≥ricas e Pr√°ticas:**

1. **Penaliza Erros Grandes:** Quadrado enfatiza desvios maiores
2. **Diferenci√°vel:** Facilita otimiza√ß√£o anal√≠tica
3. **Solu√ß√£o √önica:** Para problemas lineares
4. **Propriedades Estat√≠sticas:** Sob normalidade, √© estimador de m√°xima verossimilhan√ßa
5. **Geometria:** Proje√ß√£o ortogonal no espa√ßo vetorial

**Alternativas:**
```
M√≠nimos Quadrados:    Œ£(e·µ¢)¬≤      ‚Üê Sens√≠vel a outliers
M√≠nimos Absolutos:    Œ£|e·µ¢|       ‚Üê Mais robusto
Minimax:              max|e·µ¢|     ‚Üê Minimiza pior caso
```

### **1.3 Contexto Hist√≥rico**

**Carl Friedrich Gauss (1809):**
> Desenvolveu o m√©todo para astronomia (previs√£o de √≥rbitas de asteroides)

**Adrien-Marie Legendre (1805):**
> Publicou primeiro o m√©todo (disputa de prioridade)

**Andrey Kolmogorov (1930s):**
> Fundamenta√ß√£o probabil√≠stica moderna

---

## **2. üìä M√≠nimos Quadrados Ordin√°rios (OLS)**

### **2.1 Modelo Linear**

**Forma Escalar:**
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çöx‚Çö + Œµ

onde:
‚Ä¢ y: vari√°vel resposta
‚Ä¢ x‚±º: vari√°veis preditoras
‚Ä¢ Œ≤‚±º: coeficientes (par√¢metros)
‚Ä¢ Œµ: erro aleat√≥rio
```

**Forma Matricial:**
```
Y = XŒ≤ + Œµ

onde:
‚Ä¢ Y: vetor n√ó1 de respostas
‚Ä¢ X: matriz n√óp de preditores (design matrix)
‚Ä¢ Œ≤: vetor p√ó1 de coeficientes
‚Ä¢ Œµ: vetor n√ó1 de erros
```

**Exemplo com 3 observa√ß√µes:**
```
[y‚ÇÅ]   [1  x‚ÇÅ‚ÇÅ  x‚ÇÅ‚ÇÇ]   [Œ≤‚ÇÄ]   [Œµ‚ÇÅ]
[y‚ÇÇ] = [1  x‚ÇÇ‚ÇÅ  x‚ÇÇ‚ÇÇ] √ó [Œ≤‚ÇÅ] + [Œµ‚ÇÇ]
[y‚ÇÉ]   [1  x‚ÇÉ‚ÇÅ  x‚ÇÉ‚ÇÇ]   [Œ≤‚ÇÇ]   [Œµ‚ÇÉ]
```

### **2.2 Deriva√ß√£o da Solu√ß√£o**

**Objetivo:** Minimizar RSS (Residual Sum of Squares)
```
RSS(Œ≤) = Œ£e·µ¢¬≤ = e·µÄe = (Y - XŒ≤)·µÄ(Y - XŒ≤)
```

**Expandindo:**
```
RSS(Œ≤) = Y·µÄY - 2Œ≤·µÄX·µÄY + Œ≤·µÄX·µÄXŒ≤
```

**Derivando e igualando a zero:**
```
‚àÇRSS/‚àÇŒ≤ = -2X·µÄY + 2X·µÄXŒ≤ = 0

X·µÄXŒ≤ = X·µÄY  ‚Üê Equa√ß√µes Normais
```

**Solu√ß√£o (se X·µÄX √© invert√≠vel):**
```
Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄY
```

Esta √© a **solu√ß√£o de m√≠nimos quadrados ordin√°rios**!

### **2.3 Exemplo Num√©rico**

**Problema:** Ajustar y = Œ≤‚ÇÄ + Œ≤‚ÇÅx aos dados:
```
x: [1, 2, 3]
y: [2, 4, 5]
```

**Montando as Matrizes:**
```
X = [1  1]    Y = [2]
    [1  2]        [4]
    [1  3]        [5]
```

**Calculando X·µÄX:**
```
X·µÄX = [1  1  1] √ó [1  1]  = [3   6]
      [1  2  3]   [1  2]    [6  14]
                  [1  3]
```

**Calculando X·µÄY:**
```
X·µÄY = [1  1  1] √ó [2]  = [11]
      [1  2  3]   [4]    [27]
                  [5]
```

**Invertendo X·µÄX:**
```
det(X·µÄX) = 3√ó14 - 6√ó6 = 42 - 36 = 6

(X·µÄX)‚Åª¬π = (1/6) √ó [14  -6] = [7/3   -1]
                    [-6   3]   [-1   1/2]
```

**Calculando Œ≤ÃÇ:**
```
Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄY = [7/3   -1] √ó [11]  = [77/3 - 27]   = [50/3]   = [1.0]
                   [-1   1/2]   [27]    [-11 + 27/2]   [5.5/2]     [1.5]

Solu√ß√£o: y = 1.0 + 1.5x
```

**Verifica√ß√£o:**
```
≈∑‚ÇÅ = 1.0 + 1.5√ó1 = 2.5  (y‚ÇÅ=2, erro=0.5)
≈∑‚ÇÇ = 1.0 + 1.5√ó2 = 4.0  (y‚ÇÇ=4, erro=0.0)
≈∑‚ÇÉ = 1.0 + 1.5√ó3 = 5.5  (y‚ÇÉ=5, erro=0.5)

RSS = 0.5¬≤ + 0¬≤ + 0.5¬≤ = 0.5
```

### **2.4 Propriedades do Estimador OLS**

Sob as **hip√≥teses de Gauss-Markov**:

1. **Linearidade:** E[Œµ|X] = 0
2. **Homocedasticidade:** Var(Œµ|X) = œÉ¬≤I
3. **N√£o-correla√ß√£o:** Cov(Œµ·µ¢, Œµ‚±º) = 0 para i‚â†j

O estimador OLS √© **BLUE** (Best Linear Unbiased Estimator):
- **Best:** Menor vari√¢ncia entre estimadores lineares n√£o-viesados
- **Linear:** Combina√ß√£o linear de Y
- **Unbiased:** E[Œ≤ÃÇ] = Œ≤

**Propriedades Adicionais:**
```
E[Œ≤ÃÇ] = Œ≤                           (n√£o-viesado)
Var(Œ≤ÃÇ) = œÉ¬≤(X·µÄX)‚Åª¬π                 (matriz de covari√¢ncia)
Cov(Œ≤ÃÇ, e) = 0                      (ortogonalidade)
```

---

## **3. üîß Interpreta√ß√£o Geom√©trica**

### **3.1 Proje√ß√£o Ortogonal**

O MMQ encontra a **proje√ß√£o ortogonal** de Y no espa√ßo gerado pelas colunas de X.

**Visualiza√ß√£o (caso 2D):**
```
        Y (vetor observado)
        ‚Üë
        ‚îÇ‚ï≤
        ‚îÇ ‚ï≤ e (res√≠duo)
        ‚îÇ  ‚ï≤
        ‚îÇ   ‚Üò
        ≈∂ ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí (proje√ß√£o no espa√ßo coluna de X)
```

**Matematicamente:**
```
≈∂ = XŒ≤ÃÇ = X(X·µÄX)‚Åª¬πX·µÄY = HY

onde H = X(X·µÄX)‚Åª¬πX·µÄ √© a matriz "hat" (proje√ß√£o)
```

### **3.2 Matriz Hat (H)**

**Propriedades:**
```
1. H¬≤ = H              (idempotente)
2. H·µÄ = H              (sim√©trica)
3. HX = X              (projeta X em X)
4. trace(H) = p        (rank de H)
```

**Res√≠duos:**
```
e = Y - ≈∂ = Y - HY = (I - H)Y
```

### **3.3 Ortogonalidade**

**Propriedade Fundamental:**
```
X·µÄe = X·µÄ(Y - XŒ≤ÃÇ) = X·µÄY - X·µÄX(X·µÄX)‚Åª¬πX·µÄY = 0

Os res√≠duos s√£o ORTOGONAIS √†s colunas de X!
```

**Consequ√™ncias:**
```
1. Œ£e·µ¢ = 0              (se X tem coluna de 1s)
2. Œ£x·µ¢e·µ¢ = 0            (res√≠duos n√£o correlacionados com X)
3. Œ£≈∑·µ¢e·µ¢ = 0            (predi√ß√µes ortogonais a res√≠duos)
```

---

## **4. üìà M√≠nimos Quadrados Ponderados (WLS)**

### **4.1 Motiva√ß√£o**

Quando as observa√ß√µes t√™m **vari√¢ncias diferentes** (heterocedasticidade):
```
Var(Œµ·µ¢) = œÉ·µ¢¬≤  (n√£o constante!)
```

**Solu√ß√£o:** Dar **pesos diferentes** √†s observa√ß√µes.

### **4.2 Formula√ß√£o**

**Minimizar:**
```
S(Œ≤) = Œ£w·µ¢(y·µ¢ - f(x·µ¢, Œ≤))¬≤

onde w·µ¢ = 1/œÉ·µ¢¬≤ (inverso da vari√¢ncia)
```

**Forma Matricial:**
```
S(Œ≤) = (Y - XŒ≤)·µÄW(Y - XŒ≤)

onde W = diag(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)
```

### **4.3 Solu√ß√£o WLS**

**Equa√ß√µes Normais Ponderadas:**
```
X·µÄWXŒ≤ = X·µÄWY

Œ≤ÃÇ_WLS = (X·µÄWX)‚Åª¬πX·µÄWY
```

### **4.4 Escolha dos Pesos**

**Cen√°rios Comuns:**

1. **Vari√¢ncia Conhecida:**
   ```
   w·µ¢ = 1/œÉ·µ¢¬≤
   ```

2. **Vari√¢ncia Proporcional a x·µ¢:**
   ```
   w·µ¢ = 1/x·µ¢
   ```

3. **Contagens (Poisson):**
   ```
   w·µ¢ = 1/y·µ¢
   ```

4. **Vari√¢ncia Estimada:**
   ```
   1. Ajustar OLS
   2. Estimar œÉ·µ¢¬≤ dos res√≠duos
   3. Reajustar com WLS
   4. Iterar se necess√°rio
   ```

### **4.5 Exemplo**

**Problema:** Medi√ß√µes com precis√£o vari√°vel
```
x:      [1,    2,    3,    4,    5]
y:      [2.0,  3.8,  6.2,  7.9,  10.1]
œÉ:      [0.5,  0.5,  1.0,  1.5,  2.0]  (desvio padr√£o)

Pesos: w = 1/œÉ¬≤
w:      [4.0,  4.0,  1.0,  0.44, 0.25]
```

**Interpreta√ß√£o:** Observa√ß√µes iniciais (menor œÉ) recebem mais peso!

---

## **5. üéØ M√≠nimos Quadrados N√£o-Lineares**

### **5.1 Problema**

Quando o modelo √© **n√£o-linear nos par√¢metros**:
```
y = f(x, Œ≤) + Œµ

onde f √© n√£o-linear em Œ≤
```

**Exemplos:**
```
Exponencial:  y = Œ≤‚ÇÄe^(Œ≤‚ÇÅx)
Log√≠stica:    y = L/(1 + e^(-k(x-x‚ÇÄ)))
Michaelis-Menten: y = (V‚Çò‚Çê‚Çì√óx)/(K‚Çò + x)
```

### **5.2 M√©todos Iterativos**

N√£o h√° solu√ß√£o anal√≠tica fechada. Usa **otimiza√ß√£o iterativa**.

**Algoritmo Geral:**
```
1. Chute inicial: Œ≤‚ÅΩ‚Å∞‚Åæ
2. Para k = 0, 1, 2, ...
   a. Calcular res√≠duos: e·µ¢‚ÅΩ·µè‚Åæ = y·µ¢ - f(x·µ¢, Œ≤‚ÅΩ·µè‚Åæ)
   b. Calcular Jacobiano: J‚ÅΩ·µè‚Åæ
   c. Atualizar: Œ≤‚ÅΩ·µè‚Å∫¬π‚Åæ = Œ≤‚ÅΩ·µè‚Åæ + ŒîŒ≤‚ÅΩ·µè‚Åæ
3. Parar quando ||ŒîŒ≤‚ÅΩ·µè‚Åæ|| < tol
```

### **5.3 M√©todo de Gauss-Newton**

**Lineariza√ß√£o Local:**
```
f(x, Œ≤ + ŒîŒ≤) ‚âà f(x, Œ≤) + J√óŒîŒ≤

onde J √© o Jacobiano:
J·µ¢‚±º = ‚àÇf(x·µ¢, Œ≤)/‚àÇŒ≤‚±º
```

**Passo de Atualiza√ß√£o:**
```
ŒîŒ≤ = (J·µÄJ)‚Åª¬πJ·µÄe

(similar a OLS com J no lugar de X)
```

**Vantagens:**
- R√°pido perto da solu√ß√£o
- N√£o requer segunda derivada

**Desvantagens:**
- Pode n√£o convergir
- Sens√≠vel ao chute inicial

### **5.4 M√©todo de Levenberg-Marquardt**

**H√≠brido** entre Gauss-Newton e Gradiente Descendente:
```
(J·µÄJ + ŒªI)ŒîŒ≤ = J·µÄe

onde:
‚Ä¢ Œª = 0: Gauss-Newton (r√°pido)
‚Ä¢ Œª ‚Üí ‚àû: Gradiente Descendente (est√°vel)
```

**Estrat√©gia Adaptativa:**
```
‚Ä¢ Œª grande no in√≠cio (est√°vel)
‚Ä¢ Œª diminui √† medida que converge (r√°pido)
‚Ä¢ Se RSS aumenta: aumentar Œª, rejeitar passo
‚Ä¢ Se RSS diminui: diminuir Œª, aceitar passo
```

### **5.5 Exemplo: Ajuste Exponencial**

**Modelo:** y = ae^(bx)

**Dados:**
```
x: [0, 1, 2, 3, 4]
y: [1.0, 2.5, 7.0, 18.0, 50.0]
```

**Lineariza√ß√£o:** ln(y) = ln(a) + bx
```
Regress√£o em ln(y):
ln(y): [0, 0.92, 1.95, 2.89, 3.91]

Resultado: ln(a) ‚âà 0, b ‚âà 0.98
Logo: a ‚âà 1, b ‚âà 1

Modelo: y ‚âà e^x
```

**Refinamento com NLS:**
```
Usando Levenberg-Marquardt:
a = 0.99, b = 1.01

Modelo final: y = 0.99√óe^(1.01x)
```

---

## **6. üõ°Ô∏è Regulariza√ß√£o**

### **6.1 Problema de Overfitting**

Com muitos par√¢metros, MMQ pode **superajustar**:
```
‚Ä¢ RSS muito pequeno em treino
‚Ä¢ RSS grande em teste
‚Ä¢ Coeficientes muito grandes
```

**Solu√ß√£o:** Adicionar **penalidade** aos coeficientes.

### **6.2 Ridge Regression (L2)**

**Minimizar:**
```
S(Œ≤) = RSS + Œª√óŒ£Œ≤‚±º¬≤
     = Œ£(y·µ¢ - ≈∑·µ¢)¬≤ + Œª√ó||Œ≤||¬≤

onde Œª > 0 √© o par√¢metro de regulariza√ß√£o
```

**Solu√ß√£o:**
```
Œ≤ÃÇ_ridge = (X·µÄX + ŒªI)‚Åª¬πX·µÄY
```

**Efeitos:**
- Encolhe todos os coeficientes
- Melhora estabilidade num√©rica
- Reduz vari√¢ncia (aumenta vi√©s)
- **N√£o zera coeficientes**

**Escolha de Œª:**
```
Œª = 0:     OLS puro
Œª pequeno: Pouca regulariza√ß√£o
Œª grande:  Muita regulariza√ß√£o (Œ≤ ‚Üí 0)
Œª ‚Üí ‚àû:     Œ≤ÃÇ ‚Üí 0 (modelo constante)
```

### **6.3 Lasso Regression (L1)**

**Minimizar:**
```
S(Œ≤) = RSS + Œª√óŒ£|Œ≤‚±º|
     = Œ£(y·µ¢ - ≈∑·µ¢)¬≤ + Œª√ó||Œ≤||‚ÇÅ
```

**Caracter√≠sticas:**
- **Zera coeficientes** (sele√ß√£o de features)
- N√£o tem solu√ß√£o fechada
- Resolve por otimiza√ß√£o convexa

**Compara√ß√£o Ridge vs. Lasso:**
```
Ridge:
‚Ä¢ Encolhe suavemente
‚Ä¢ Mant√©m todas features
‚Ä¢ Bom quando muitas features relevantes

Lasso:
‚Ä¢ Seleciona features (zera coeficientes)
‚Ä¢ Interpret√°vel
‚Ä¢ Bom quando muitas features irrelevantes
```

### **6.4 Elastic Net**

**Combina√ß√£o** de Ridge e Lasso:
```
S(Œ≤) = RSS + Œª‚ÇÅ√óŒ£|Œ≤‚±º| + Œª‚ÇÇ√óŒ£Œ≤‚±º¬≤
```

**Vantagens:**
- Herda benef√≠cios de ambos
- Est√°vel com features correlacionadas
- Sele√ß√£o de grupos de features

---

## **7. üìä Diagn√≥sticos e Valida√ß√£o**

### **7.1 An√°lise de Res√≠duos**

**Res√≠duos Padronizados:**
```
r·µ¢ = e·µ¢ / (s√ó‚àö(1 - h·µ¢·µ¢))

onde:
‚Ä¢ s = ‚àö(RSS/(n-p)): desvio padr√£o residual
‚Ä¢ h·µ¢·µ¢: elemento diagonal de H (leverage)
```

**Gr√°ficos:**
1. **Res√≠duos vs. Ajustados:** Detecta n√£o-linearidade
2. **Q-Q Plot:** Testa normalidade
3. **Scale-Location:** Detecta heterocedasticidade
4. **Residuals vs. Leverage:** Identifica pontos influentes

### **7.2 Estat√≠sticas de Influ√™ncia**

**Leverage (h·µ¢·µ¢):**
```
Mede qu√£o "extremo" √© x·µ¢

h·µ¢·µ¢ alto ‚Üí ponto influente
```

**Dist√¢ncia de Cook:**
```
D·µ¢ = (r·µ¢¬≤/p) √ó (h·µ¢·µ¢/(1-h·µ¢·µ¢))

D·µ¢ > 1: ponto muito influente (investigar)
```

**DFBETAS:**
```
Mudan√ßa em Œ≤ ao remover observa√ß√£o i
```

### **7.3 M√©tricas de Qualidade**

**R¬≤ (Coeficiente de Determina√ß√£o):**
```
R¬≤ = 1 - RSS/TSS

0 ‚â§ R¬≤ ‚â§ 1
```

**R¬≤ Ajustado:**
```
R¬≤‚Çê‚±º = 1 - (1-R¬≤)√ó(n-1)/(n-p-1)

Penaliza modelos com muitos par√¢metros
```

**AIC (Akaike Information Criterion):**
```
AIC = n√óln(RSS/n) + 2p

Menor AIC = melhor modelo
```

**BIC (Bayesian Information Criterion):**
```
BIC = n√óln(RSS/n) + p√óln(n)

Penaliza mais modelos complexos que AIC
```

---

## **8. üßÆ Considera√ß√µes Computacionais**

### **8.1 Complexidade**

**M√©todo Direto (Invers√£o):**
```
Complexidade: O(p¬≥) + O(p¬≤n)

Gargalo: invers√£o de (X·µÄX)
```

**Decomposi√ß√£o QR:**
```
X = QR

Œ≤ÃÇ = R‚Åª¬πQ·µÄY

Complexidade: O(p¬≤n)
Mais est√°vel numericamente
```

**Decomposi√ß√£o SVD:**
```
X = UŒ£V·µÄ

Œ≤ÃÇ = VŒ£‚Å∫U·µÄY

onde Œ£‚Å∫ √© pseudo-inversa

Mais est√°vel, detecta multicolinearidade
```

### **8.2 Problemas Num√©ricos**

**Multicolinearidade:**
```
Colunas de X altamente correlacionadas

Problema: (X·µÄX) quase singular
Solu√ß√£o: Ridge, PCA, remover features
```

**N√∫mero de Condi√ß√£o:**
```
Œ∫ = œÉ‚Çò‚Çê‚Çì / œÉ‚Çò·µ¢‚Çô

Œ∫ > 100: mal-condicionado
Œ∫ > 1000: muito mal-condicionado
```

**Rank Deficiency:**
```
rank(X) < p (colunas linearmente dependentes)

Solu√ß√£o: Pseudo-inversa (SVD)
```

### **8.3 Grandes Datasets**

**Gradiente Descendente Estoc√°stico (SGD):**
```
Para cada mini-batch:
  Œ≤ ‚Üê Œ≤ - Œ±√ó‚àáRSS

Complexidade por itera√ß√£o: O(batch_size √ó p)
```

**Online Learning:**
```
Atualiza Œ≤ incrementalmente com novos dados
N√£o precisa armazenar todos os dados
```

---

## **9. üßÆ Exerc√≠cios Resolvidos**

### **Exerc√≠cio 1: OLS Manual**
**Dados:** Ajustar y = Œ≤‚ÇÄ + Œ≤‚ÇÅx
```
x: [0, 1, 2]
y: [1, 2, 4]
```

**Solu√ß√£o:**
```
X = [1  0]    Y = [1]
    [1  1]        [2]
    [1  2]        [4]

X·µÄX = [3  3]    X·µÄY = [7]
      [3  5]          [10]

det = 15 - 9 = 6

(X·µÄX)‚Åª¬π = (1/6)[5  -3] = [5/6   -1/2]
               [-3  3]    [-1/2   1/2]

Œ≤ÃÇ = [5/6   -1/2] √ó [7]  = [35/6 - 5]   = [5/6]   ‚âà [0.83]
    [-1/2   1/2]   [10]    [-7/2 + 5]     [3/2]     [1.50]

Modelo: y = 0.83 + 1.50x
```

### **Exerc√≠cio 2: Res√≠duos e R¬≤**
**Continuando Exerc√≠cio 1:**

**Predi√ß√µes:**
```
≈∑‚ÇÄ = 0.83 + 1.50√ó0 = 0.83
≈∑‚ÇÅ = 0.83 + 1.50√ó1 = 2.33
≈∑‚ÇÇ = 0.83 + 1.50√ó2 = 3.83
```

**Res√≠duos:**
```
e‚ÇÄ = 1 - 0.83 = 0.17
e‚ÇÅ = 2 - 2.33 = -0.33
e‚ÇÇ = 4 - 3.83 = 0.17

RSS = 0.17¬≤ + 0.33¬≤ + 0.17¬≤ = 0.167
```

**R¬≤:**
```
»≥ = 7/3 ‚âà 2.33

TSS = (1-2.33)¬≤ + (2-2.33)¬≤ + (4-2.33)¬≤
    = 1.78 + 0.11 + 2.78 = 4.67

R¬≤ = 1 - 0.167/4.67 = 1 - 0.036 = 0.964 = 96.4%
```

---

## **10. üíª Implementa√ß√£o em Python**

```python
import numpy as np
from scipy.linalg import lstsq
from sklearn.linear_model import LinearRegression, Ridge, Lasso
import matplotlib.pyplot as plt

# Dados
X = np.array([[1, 0], [1, 1], [1, 2]])
y = np.array([1, 2, 4])

# 1. Solu√ß√£o Manual (OLS)
XtX = X.T @ X
Xty = X.T @ y
beta_manual = np.linalg.inv(XtX) @ Xty
print(f"OLS Manual: {beta_manual}")

# 2. Usando scipy.linalg.lstsq
beta_scipy, residuals, rank, s = lstsq(X, y)
print(f"OLS scipy: {beta_scipy}")

# 3. Usando sklearn
model = LinearRegression()
model.fit(X[:, 1:], y)  # sem coluna de 1s
print(f"OLS sklearn: intercept={model.intercept_}, coef={model.coef_}")

# 4. Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X[:, 1:], y)
print(f"Ridge: intercept={ridge.intercept_}, coef={ridge.coef_}")

# 5. Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(X[:, 1:], y)
print(f"Lasso: intercept={lasso.intercept_}, coef={lasso.coef_}")

# 6. An√°lise de Res√≠duos
y_pred = X @ beta_manual
residuals = y - y_pred
RSS = np.sum(residuals**2)
TSS = np.sum((y - y.mean())**2)
R2 = 1 - RSS/TSS

print(f"\nAn√°lise:")
print(f"Res√≠duos: {residuals}")
print(f"RSS: {RSS:.4f}")
print(f"R¬≤: {R2:.4f}")

# 7. Visualiza√ß√£o
plt.scatter(X[:, 1], y, label='Dados', s=100)
x_plot = np.linspace(0, 2, 100)
y_plot = beta_manual[0] + beta_manual[1]*x_plot
plt.plot(x_plot, y_plot, 'r-', label='Ajuste OLS')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title(f'Regress√£o Linear: y = {beta_manual[0]:.2f} + {beta_manual[1]:.2f}x')
plt.grid(True)
plt.show()
```

---

## **11. üîó Recursos Adicionais**

### **Livros Recomendados**
- **Linear Regression Analysis** - Seber & Lee
- **Applied Linear Regression** - Weisberg
- **Matrix Computations** - Golub & Van Loan
- **Numerical Recipes** - Press et al.

### **Bibliotecas Python**
```python
# B√°sico
import numpy as np
from scipy import linalg, optimize

# Machine Learning
from sklearn.linear_model import (
    LinearRegression,  # OLS
    Ridge,            # L2
    Lasso,            # L1
    ElasticNet,       # L1 + L2
    HuberRegressor    # Robusto
)

# Estat√≠stica
import statsmodels.api as sm
from statsmodels.regression.linear_model import OLS, WLS
```

### **Ferramentas Online**
- [Wolfram Alpha](https://www.wolframalpha.com/)
- [Matrix Calculator](https://matrixcalc.org/)
- [Desmos Regression](https://www.desmos.com/calculator)

---

**Voltar para:** [Estat√≠stica](../README.md) | [Notebooks](../../README.md)
