# Ajuste de Curva üìà

**Ajuste de curva** (curve fitting) √© o processo de encontrar uma fun√ß√£o matem√°tica que melhor representa a rela√ß√£o entre vari√°veis observadas. √â fundamental em ci√™ncia de dados, modelagem estat√≠stica e aprendizado de m√°quina.

---

## **1. üéØ Fundamentos Te√≥ricos**

### **1.1 O Que √â Ajuste de Curva?**

Dado um conjunto de pontos de dados (x, y), queremos encontrar uma fun√ß√£o f(x) tal que:
```
y ‚âà f(x)
```

**Objetivo:**
> Encontrar a fun√ß√£o que **minimize o erro** entre valores observados e preditos.

**Tipos de Ajuste:**
- **Regress√£o:** Ajuste estat√≠stico com ru√≠do
- **Interpola√ß√£o:** Passa exatamente pelos pontos
- **Suaviza√ß√£o:** Compromisso entre ambos

### **1.2 Por Que Ajustar Curvas?**

**Aplica√ß√µes:**
- ‚úÖ **Predi√ß√£o:** Estimar valores futuros
- ‚úÖ **Modelagem:** Descrever rela√ß√µes f√≠sicas/naturais
- ‚úÖ **Compress√£o:** Representar muitos pontos com poucos par√¢metros
- ‚úÖ **An√°lise:** Identificar tend√™ncias e padr√µes
- ‚úÖ **Interpola√ß√£o:** Estimar valores entre pontos conhecidos

---

## **2. üìä Regress√£o Linear Simples**

### **2.1 Defini√ß√£o**

Ajusta uma **reta** aos dados:
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ

onde:
‚Ä¢ Œ≤‚ÇÄ: intercepto (coeficiente linear)
‚Ä¢ Œ≤‚ÇÅ: inclina√ß√£o (coeficiente angular)
‚Ä¢ Œµ: erro aleat√≥rio
```

**Forma Estimada:**
```
≈∑ = b‚ÇÄ + b‚ÇÅx

onde:
‚Ä¢ ≈∑ (y-chap√©u): valor predito
‚Ä¢ b‚ÇÄ, b‚ÇÅ: estimativas dos par√¢metros
```

### **2.2 M√©todo dos M√≠nimos Quadrados Ordin√°rios (OLS)**

Minimiza a **soma dos quadrados dos res√≠duos (RSS)**:
```
RSS = Œ£(y·µ¢ - ≈∑·µ¢)¬≤ = Œ£(y·µ¢ - b‚ÇÄ - b‚ÇÅx·µ¢)¬≤
```

**Solu√ß√£o Anal√≠tica:**
```
b‚ÇÅ = Œ£(x·µ¢ - xÃÑ)(y·µ¢ - »≥) / Œ£(x·µ¢ - xÃÑ)¬≤
   = Cov(X,Y) / Var(X)

b‚ÇÄ = »≥ - b‚ÇÅxÃÑ
```

### **2.3 Exemplo Pr√°tico**

**Dados:** Horas de estudo vs. Nota
```
X (horas): [1, 2, 3, 4, 5]
Y (nota):  [50, 60, 65, 80, 85]
```

**C√°lculo:**
```
Passo 1: M√©dias
xÃÑ = 3, »≥ = 68

Passo 2: Calcular b‚ÇÅ
Numerador: (1-3)(50-68) + (2-3)(60-68) + ... = 70
Denominador: (1-3)¬≤ + (2-3)¬≤ + ... = 10

b‚ÇÅ = 70/10 = 7

Passo 3: Calcular b‚ÇÄ
b‚ÇÄ = 68 - 7√ó3 = 68 - 21 = 47

Equa√ß√£o: ≈∑ = 47 + 7x
```

**Interpreta√ß√£o:**
```
‚Ä¢ Nota inicial (sem estudo): 47
‚Ä¢ Cada hora adicional aumenta nota em 7 pontos
```

### **2.4 Propriedades**

**Propriedade 1: Reta Passa Pela M√©dia**
```
A reta sempre passa pelo ponto (xÃÑ, »≥)
```

**Propriedade 2: Res√≠duos**
```
‚Ä¢ Soma dos res√≠duos = 0
‚Ä¢ Res√≠duos n√£o correlacionados com X (OLS)
```

**Propriedade 3: Decomposi√ß√£o da Vari√¢ncia**
```
TSS = RSS + ESS

onde:
‚Ä¢ TSS: Varia√ß√£o total = Œ£(y·µ¢ - »≥)¬≤
‚Ä¢ RSS: Varia√ß√£o residual = Œ£(y·µ¢ - ≈∑·µ¢)¬≤
‚Ä¢ ESS: Varia√ß√£o explicada = Œ£(≈∑·µ¢ - »≥)¬≤
```

### **2.5 Coeficiente de Determina√ß√£o (R¬≤)**

Mede a **propor√ß√£o da vari√¢ncia explicada** pelo modelo.

**F√≥rmula:**
```
R¬≤ = 1 - (RSS / TSS) = ESS / TSS

0 ‚â§ R¬≤ ‚â§ 1
```

**Interpreta√ß√£o:**
```
R¬≤ = 0.0:   Modelo n√£o explica nada (linha horizontal)
R¬≤ = 0.5:   Modelo explica 50% da vari√¢ncia
R¬≤ = 1.0:   Ajuste perfeito (todos pontos na reta)
```

**Classifica√ß√£o:**
```
R¬≤ < 0.3:     Ajuste fraco
0.3 ‚â§ R¬≤ < 0.7: Ajuste moderado
R¬≤ ‚â• 0.7:     Ajuste forte
```

**Exemplo:**
```
TSS = 1000
RSS = 200

R¬≤ = 1 - 200/1000 = 0.8 = 80%

Interpreta√ß√£o: Modelo explica 80% da varia√ß√£o nas notas!
```

### **2.6 Hip√≥teses do Modelo Linear**

**Para infer√™ncia v√°lida:**
1. **Linearidade:** Rela√ß√£o √© linear
2. **Independ√™ncia:** Observa√ß√µes independentes
3. **Homocedasticidade:** Vari√¢ncia constante dos erros
4. **Normalidade:** Erros seguem distribui√ß√£o normal
5. **Sem multicolinearidade:** (regress√£o m√∫ltipla)

---

## **3. üìà Regress√£o Linear M√∫ltipla**

### **3.1 Defini√ß√£o**

Modelo com **m√∫ltiplas vari√°veis preditoras**:
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çöx‚Çö + Œµ
```

**Forma Matricial:**
```
Y = XŒ≤ + Œµ

onde:
‚Ä¢ Y: vetor n√ó1 de respostas
‚Ä¢ X: matriz n√ó(p+1) de preditores
‚Ä¢ Œ≤: vetor (p+1)√ó1 de coeficientes
‚Ä¢ Œµ: vetor n√ó1 de erros
```

### **3.2 Solu√ß√£o dos M√≠nimos Quadrados**

**Equa√ß√£o Normal:**
```
Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄY
```

**Interpreta√ß√£o dos Coeficientes:**
```
Œ≤‚±º: mudan√ßa em Y para aumento unit√°rio em x‚±º,
    mantendo todas outras vari√°veis constantes
```

### **3.3 Exemplo**

**Predizer pre√ßo de casa:**
```
Pre√ßo = Œ≤‚ÇÄ + Œ≤‚ÇÅ√ó√Årea + Œ≤‚ÇÇ√óQuartos + Œ≤‚ÇÉ√óIdade
```

**Resultados Hipot√©ticos:**
```
Pre√ßo = 50000 + 1000√ó√Årea + 20000√óQuartos - 500√óIdade

Interpreta√ß√£o:
‚Ä¢ Casa base: R$ 50.000
‚Ä¢ Cada m¬≤ adicional: +R$ 1.000
‚Ä¢ Cada quarto adicional: +R$ 20.000
‚Ä¢ Cada ano mais velha: -R$ 500
```

### **3.4 R¬≤ Ajustado**

Penaliza adi√ß√£o de vari√°veis irrelevantes:
```
R¬≤‚Çê‚±º = 1 - (1-R¬≤)√ó(n-1)/(n-p-1)

onde:
‚Ä¢ n: n√∫mero de observa√ß√µes
‚Ä¢ p: n√∫mero de preditores
```

**Por que usar?**
> R¬≤ sempre aumenta ao adicionar vari√°veis, mesmo irrelevantes. R¬≤‚Çê‚±º s√≥ aumenta se nova vari√°vel melhora significativamente o modelo.

---

## **4. üîÑ Regress√£o Polinomial**

### **4.1 Defini√ß√£o**

Ajusta **polin√¥mios** aos dados:
```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œ≤‚ÇÇx¬≤ + Œ≤‚ÇÉx¬≥ + ... + Œ≤‚Çôx‚Åø + Œµ
```

**Graus Comuns:**
- **Grau 1:** Linear (reta)
- **Grau 2:** Quadr√°tico (par√°bola)
- **Grau 3:** C√∫bico
- **Grau n:** Polin√¥mio de ordem n

### **4.2 Quando Usar?**

**Indicadores de N√£o-Linearidade:**
```
‚Ä¢ Gr√°fico de res√≠duos mostra padr√£o
‚Ä¢ Rela√ß√£o claramente curva
‚Ä¢ Conhecimento do dom√≠nio sugere n√£o-linearidade
```

**Exemplo Visual:**
```
Linear (ruim):          Quadr√°tico (melhor):
    ‚óè                       ‚óè
   ‚óè                       ‚óè  ‚ï±‚ï≤
  ‚óè  /                    ‚óè  ‚ï±  ‚ï≤
 ‚óè  /                    ‚óè  ‚ï±    ‚ï≤
‚óè  /                    ‚óè  ‚ï±      ‚ï≤
```

### **4.3 Escolha do Grau**

**Trade-off:**
```
Grau baixo ‚Üí Underfitting (subajuste)
Grau alto  ‚Üí Overfitting (sobreajuste)
```

**M√©todos de Sele√ß√£o:**
1. **Valida√ß√£o Cruzada:** Erro em dados de teste
2. **Crit√©rios de Informa√ß√£o:** AIC, BIC
3. **Conhecimento do Dom√≠nio:** Base te√≥rica
4. **An√°lise de Res√≠duos:** Padr√µes residuais

**Exemplo:**
```
Dados com 10 pontos:
‚Ä¢ Grau 1: R¬≤ = 0.70  ‚Üê Pode estar subajustado
‚Ä¢ Grau 2: R¬≤ = 0.92  ‚Üê Bom equil√≠brio
‚Ä¢ Grau 3: R¬≤ = 0.94  ‚Üê Melhora marginal
‚Ä¢ Grau 9: R¬≤ = 1.00  ‚Üê Passa por todos, mas overfit!
```

### **4.4 Implementa√ß√£o**

**Transforma√ß√£o:**
```python
# Criar features polinomiais
X_poly = [x, x¬≤, x¬≥, ...]

# Depois aplicar regress√£o linear
y = Œ≤‚ÇÄ + Œ≤‚ÇÅ√óX_poly[:, 0] + Œ≤‚ÇÇ√óX_poly[:, 1] + ...
```

**Nota:** Regress√£o polinomial √© **linear nos par√¢metros**, ent√£o usamos m√≠nimos quadrados lineares!

### **4.5 Problemas Potenciais**

**Oscila√ß√µes de Runge:**
```
Polin√¥mios de alto grau podem oscilar
violentamente entre pontos de dados
```

**Extrapola√ß√£o Perigosa:**
```
Polin√¥mios podem divergir rapidamente
fora do intervalo de dados observados
```

**Multicolinearidade:**
```
x, x¬≤, x¬≥, ... s√£o altamente correlacionados
Solu√ß√£o: Usar polin√¥mios ortogonais
```

---

## **5. üîß Interpola√ß√£o**

### **5.1 Diferen√ßa: Interpola√ß√£o vs. Regress√£o**

| **Aspecto** | **Interpola√ß√£o** | **Regress√£o** |
|-------------|------------------|---------------|
| **Passa pelos pontos** | ‚úÖ Exato | ‚ùå Aproximado |
| **Considera ru√≠do** | ‚ùå N√£o | ‚úÖ Sim |
| **Suaviza√ß√£o** | N√£o | Sim |
| **Graus de liberdade** | n-1 (n pontos) | p (par√¢metros) |
| **Uso** | Dados sem ru√≠do | Dados com ru√≠do |

### **5.2 Interpola√ß√£o Linear por Partes**

Conecta pontos consecutivos com **segmentos de reta**.

**Vantagens:**
- ‚úÖ Simples e r√°pido
- ‚úÖ Sempre funciona
- ‚úÖ N√£o oscila

**Desvantagens:**
- ‚ùå N√£o diferenci√°vel nos n√≥s
- ‚ùå Apar√™ncia "quebrada"

### **5.3 Interpola√ß√£o de Lagrange**

**F√≥rmula:**
```
P(x) = Œ£ y·µ¢ √ó L·µ¢(x)

onde:
L·µ¢(x) = ‚àè (x - x‚±º) / (x·µ¢ - x‚±º)  para j‚â†i
        j
```

**Caracter√≠sticas:**
- Polin√¥mio de grau n-1 para n pontos
- Passa exatamente por todos os pontos
- Pode oscilar para muitos pontos

**Exemplo (2 pontos):**
```
Pontos: (1, 2), (3, 8)

L‚ÇÅ(x) = (x - 3) / (1 - 3) = (x - 3) / (-2)
L‚ÇÇ(x) = (x - 1) / (3 - 1) = (x - 1) / 2

P(x) = 2√óL‚ÇÅ(x) + 8√óL‚ÇÇ(x)
     = 2√ó(x-3)/(-2) + 8√ó(x-1)/2
     = -(x-3) + 4(x-1)
     = -x + 3 + 4x - 4
     = 3x - 1

Verifica√ß√£o:
P(1) = 3√ó1 - 1 = 2 ‚úì
P(3) = 3√ó3 - 1 = 8 ‚úì
```

### **5.4 Splines C√∫bicos**

**Polin√¥mio c√∫bico por partes** que √© suave nos n√≥s.

**Propriedades:**
- C√∫bico entre cada par de pontos consecutivos
- Fun√ß√£o cont√≠nua (C‚Å∞)
- Primeira derivada cont√≠nua (C¬π)
- Segunda derivada cont√≠nua (C¬≤)

**Vantagens:**
- ‚úÖ **Suave** (C¬≤ cont√≠nuo)
- ‚úÖ N√£o oscila como polin√¥mios de alto grau
- ‚úÖ Flex√≠vel e preciso
- ‚úÖ Padr√£o em gr√°ficos e CAD

**Tipos:**
- **Natural Spline:** Segunda derivada = 0 nas extremidades
- **Clamped Spline:** Primeira derivada especificada nas extremidades
- **Not-a-Knot:** Terceira derivada cont√≠nua em x‚ÇÇ e x‚Çô‚Çã‚ÇÅ

### **5.5 Aplica√ß√µes de Interpola√ß√£o**

- **Gr√°ficos:** Suaviza√ß√£o de curvas
- **Anima√ß√£o:** Interpola√ß√£o de keyframes
- **Processamento de Sinais:** Reamostragem
- **CAD/CAM:** Design de curvas suaves
- **Geof√≠sica:** Interpola√ß√£o de dados espaciais

---

## **6. üìä Avalia√ß√£o de Modelos**

### **6.1 M√©tricas de Erro**

**Mean Squared Error (MSE):**
```
MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤

Penaliza erros grandes quadraticamente
```

**Root Mean Squared Error (RMSE):**
```
RMSE = ‚àöMSE

Mesma unidade que Y, mais interpret√°vel
```

**Mean Absolute Error (MAE):**
```
MAE = (1/n) √ó Œ£|y·µ¢ - ≈∑·µ¢|

Menos sens√≠vel a outliers que MSE
```

**Mean Absolute Percentage Error (MAPE):**
```
MAPE = (100/n) √ó Œ£|y·µ¢ - ≈∑·µ¢| / |y·µ¢|

Erro relativo em percentual
```

**Compara√ß√£o:**
```
Dados: y = [100, 110, 120]
Predi√ß√µes: ≈∑ = [98, 115, 118]

Erros: [-2, 5, -2]

MSE = (4 + 25 + 4) / 3 = 11
RMSE = ‚àö11 ‚âà 3.32
MAE = (2 + 5 + 2) / 3 = 3
MAPE = (2/100 + 5/110 + 2/120) √ó 100/3 ‚âà 2.1%
```

### **6.2 An√°lise de Res√≠duos**

**Res√≠duos:**
```
e·µ¢ = y·µ¢ - ≈∑·µ¢
```

**Gr√°ficos Diagn√≥sticos:**

**1. Res√≠duos vs. Valores Ajustados**
```
Ideal: pontos aleat√≥rios em torno de zero
Problema: padr√£o indica n√£o-linearidade
```

**2. Q-Q Plot**
```
Ideal: pontos na diagonal
Problema: desvios indicam n√£o-normalidade
```

**3. Res√≠duos vs. Leverage**
```
Identifica pontos influentes
```

### **6.3 Valida√ß√£o**

**Valida√ß√£o Holdout:**
```
‚Ä¢ 70-80% dados de treino
‚Ä¢ 20-30% dados de teste
‚Ä¢ Avaliar em dados n√£o vistos
```

**Valida√ß√£o Cruzada K-Fold:**
```
‚Ä¢ Dividir dados em K partes
‚Ä¢ Treinar em K-1, testar em 1
‚Ä¢ Repetir K vezes
‚Ä¢ M√©dia dos erros
```

**Leave-One-Out (LOO):**
```
‚Ä¢ K = n (cada ponto √© fold)
‚Ä¢ M√°xima utiliza√ß√£o dos dados
‚Ä¢ Computacionalmente caro
```

---

## **7. üéØ Modelos N√£o-Lineares**

### **7.1 Regress√£o N√£o-Linear**

Quando a rela√ß√£o n√£o √© linear nos **par√¢metros**:
```
y = f(x, Œ≤) + Œµ

onde f n√£o √© linear em Œ≤
```

**Exemplos:**
```
Exponencial:  y = Œ≤‚ÇÄ √ó e^(Œ≤‚ÇÅx)
Log√≠stica:    y = L / (1 + e^(-k(x-x‚ÇÄ)))
Pot√™ncia:     y = Œ≤‚ÇÄ √ó x^Œ≤‚ÇÅ
```

### **7.2 M√©todo de Otimiza√ß√£o**

N√£o h√° solu√ß√£o anal√≠tica, usa **otimiza√ß√£o iterativa**:

**M√©todos Comuns:**
- **Gradiente Descendente**
- **Levenberg-Marquardt**
- **Gauss-Newton**
- **Trust Region**

**Processo:**
```
1. Chute inicial para Œ≤
2. Calcular erro (RSS)
3. Atualizar Œ≤ para reduzir erro
4. Repetir at√© converg√™ncia
```

### **7.3 Exemplo: Crescimento Exponencial**

**Modelo:**
```
y = a √ó e^(bx)
```

**Lineariza√ß√£o (truque):**
```
log(y) = log(a) + bx

Fica linear em log(y)!
Aplicar regress√£o linear em log(y)
```

**Dados:**
```
x: [0, 1, 2, 3, 4]
y: [1, 2.7, 7.4, 20.1, 54.6]
```

**Solu√ß√£o:**
```
log(y): [0, 1, 2, 3, 4]  (aproximadamente)

Regress√£o: log(y) = 0 + 1√óx
Portanto: y = e^0 √ó e^(1√óx) = e^x

Verifica√ß√£o:
e^0 = 1 ‚úì
e^1 ‚âà 2.7 ‚úì
e^2 ‚âà 7.4 ‚úì
...
```

---

## **8. üöÄ T√©cnicas Avan√ßadas**

### **8.1 Regulariza√ß√£o**

Adiciona **penalidade** aos coeficientes para prevenir overfitting.

**Ridge Regression (L2):**
```
Minimizar: RSS + Œª √ó Œ£Œ≤‚±º¬≤

Encolhe coeficientes suavemente
```

**Lasso Regression (L1):**
```
Minimizar: RSS + Œª √ó Œ£|Œ≤‚±º|

For√ßa alguns coeficientes a zero (sele√ß√£o de features)
```

**Elastic Net:**
```
Minimizar: RSS + Œª‚ÇÅ√óŒ£|Œ≤‚±º| + Œª‚ÇÇ√óŒ£Œ≤‚±º¬≤

Combina√ß√£o de Ridge e Lasso
```

### **8.2 Regress√£o Robusta**

Menos sens√≠vel a **outliers**.

**M√©todos:**
- **Least Absolute Deviations (LAD):** Minimiza MAE
- **Huber Regression:** H√≠brido quadr√°tico/absoluto
- **RANSAC:** Amostra aleat√≥ria de consenso
- **Theil-Sen:** Mediana das inclina√ß√µes

### **8.3 Regress√£o N√£o-Param√©trica**

N√£o assume forma funcional espec√≠fica.

**M√©todos:**
- **Loess/Lowess:** Regress√£o local ponderada
- **Spline Smoothing:** Splines com regulariza√ß√£o
- **Kernel Regression:** M√©dia ponderada local
- **Gaussian Process Regression:** Distribui√ß√£o sobre fun√ß√µes

---

## **9. üßÆ Exerc√≠cios Resolvidos**

### **Exerc√≠cio 1: Regress√£o Linear Simples**
**Dados:** X = [1, 2, 3, 4], Y = [2, 4, 5, 4]

**Solu√ß√£o:**
```
xÃÑ = 2.5, »≥ = 3.75

b‚ÇÅ = [(1-2.5)(2-3.75) + ... ] / [(1-2.5)¬≤ + ...]
   = 5.5 / 5 = 1.1

b‚ÇÄ = 3.75 - 1.1√ó2.5 = 1.0

Equa√ß√£o: ≈∑ = 1.0 + 1.1x

Predi√ß√£o para x=5:
≈∑ = 1.0 + 1.1√ó5 = 6.5
```

### **Exerc√≠cio 2: R¬≤**
**Problema:** Calcular R¬≤ para:
```
Y observado: [2, 4, 5, 4]
≈∂ predito:   [2.1, 3.2, 4.3, 5.4]
```

**Solu√ß√£o:**
```
»≥ = 3.75

TSS = (2-3.75)¬≤ + (4-3.75)¬≤ + (5-3.75)¬≤ + (4-3.75)¬≤
    = 3.0625 + 0.0625 + 1.5625 + 0.0625 = 4.75

RSS = (2-2.1)¬≤ + (4-3.2)¬≤ + (5-4.3)¬≤ + (4-5.4)¬≤
    = 0.01 + 0.64 + 0.49 + 1.96 = 3.1

R¬≤ = 1 - 3.1/4.75 = 1 - 0.653 = 0.347 ‚âà 34.7%
```

---

## **10. üíª Implementa√ß√£o em Python**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
from scipy.interpolate import CubicSpline

# Dados de exemplo
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([2, 4, 5, 4, 5])

# 1. Regress√£o Linear
model_linear = LinearRegression()
model_linear.fit(X, y)
y_pred_linear = model_linear.predict(X)

print(f"Coeficientes: Œ≤‚ÇÄ={model_linear.intercept_:.2f}, Œ≤‚ÇÅ={model_linear.coef_[0]:.2f}")
print(f"R¬≤ Linear: {r2_score(y, y_pred_linear):.3f}")

# 2. Regress√£o Polinomial (grau 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model_poly = LinearRegression()
model_poly.fit(X_poly, y)
y_pred_poly = model_poly.predict(X_poly)

print(f"R¬≤ Polinomial: {r2_score(y, y_pred_poly):.3f}")

# 3. Interpola√ß√£o com Spline C√∫bico
cs = CubicSpline(X.ravel(), y)
X_smooth = np.linspace(1, 5, 100)
y_smooth = cs(X_smooth)

# 4. Visualiza√ß√£o
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.scatter(X, y, label='Dados')
plt.plot(X, y_pred_linear, 'r-', label='Linear')
plt.legend()
plt.title('Regress√£o Linear')

plt.subplot(1, 3, 2)
plt.scatter(X, y, label='Dados')
plt.plot(X, y_pred_poly, 'g-', label='Polinomial (grau 2)')
plt.legend()
plt.title('Regress√£o Polinomial')

plt.subplot(1, 3, 3)
plt.scatter(X, y, label='Dados')
plt.plot(X_smooth, y_smooth, 'b-', label='Spline C√∫bico')
plt.legend()
plt.title('Interpola√ß√£o Spline')

plt.tight_layout()
plt.show()
```

---

## **11. üîó Recursos Adicionais**

### **Livros Recomendados**
- **Introduction to Statistical Learning** - James et al.
- **The Elements of Statistical Learning** - Hastie, Tibshirani & Friedman
- **Numerical Methods** - Press et al.
- **Applied Regression Analysis** - Draper & Smith

### **Ferramentas Online**
- [Curve Fitting Tool](https://www.desmos.com/calculator)
- [Wolfram Alpha](https://www.wolframalpha.com/)
- [GeoGebra](https://www.geogebra.org/)

### **Bibliotecas Python**
```python
# Regress√£o
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from statsmodels.api import OLS

# Interpola√ß√£o
from scipy.interpolate import interp1d, CubicSpline, UnivariateSpline

# Otimiza√ß√£o
from scipy.optimize import curve_fit, least_squares
```

---

**Voltar para:** [Estat√≠stica](../README.md) | [Notebooks](../../README.md)
