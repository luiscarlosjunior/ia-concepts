# M√©todos Num√©ricos - Sistemas Lineares üî¢

A resolu√ß√£o de **sistemas de equa√ß√µes lineares** √© um problema fundamental em matem√°tica computacional, engenharia, ci√™ncia de dados e aprendizado de m√°quina. Este documento apresenta os principais m√©todos num√©ricos para resolver sistemas lineares Ax = b.

---

## **1. üéØ Fundamentos Te√≥ricos**

### **1.1 O Problema**

**Sistema de Equa√ß√µes Lineares:**
```
a‚ÇÅ‚ÇÅx‚ÇÅ + a‚ÇÅ‚ÇÇx‚ÇÇ + ... + a‚ÇÅ‚Çôx‚Çô = b‚ÇÅ
a‚ÇÇ‚ÇÅx‚ÇÅ + a‚ÇÇ‚ÇÇx‚ÇÇ + ... + a‚ÇÇ‚Çôx‚Çô = b‚ÇÇ
  ‚ãÆ       ‚ãÆ             ‚ãÆ      ‚ãÆ
a‚Çò‚ÇÅx‚ÇÅ + a‚Çò‚ÇÇx‚ÇÇ + ... + a‚Çò‚Çôx‚Çô = b‚Çò
```

**Forma Matricial:**
```
Ax = b

onde:
‚Ä¢ A: matriz m√ón de coeficientes
‚Ä¢ x: vetor n√ó1 de inc√≥gnitas (solu√ß√£o)
‚Ä¢ b: vetor m√ó1 de termos independentes
```

**Exemplo:**
```
2x + 3y = 8
4x - y = 2

Em forma matricial:
[2   3] [x]   [8]
[4  -1] [y] = [2]
```

### **1.2 Tipos de Sistemas**

**Por Dimens√£o:**
```
m = n: Sistema quadrado (n equa√ß√µes, n inc√≥gnitas)
m > n: Sistema sobredeterminado (mais equa√ß√µes que inc√≥gnitas)
m < n: Sistema subdeterminado (menos equa√ß√µes que inc√≥gnitas)
```

**Por Solu√ß√£o:**
```
‚Ä¢ Consistente: tem pelo menos uma solu√ß√£o
  - Determinado: solu√ß√£o √∫nica
  - Indeterminado: infinitas solu√ß√µes
‚Ä¢ Inconsistente: sem solu√ß√£o
```

**Condi√ß√µes de Exist√™ncia (Sistema Quadrado):**
```
det(A) ‚â† 0: Solu√ß√£o √∫nica (sistema n√£o-singular)
det(A) = 0: Sem solu√ß√£o ou infinitas solu√ß√µes (singular)
```

### **1.3 Por Que M√©todos Num√©ricos?**

**Limita√ß√µes Anal√≠ticas:**
- Sistemas grandes (n > 1000)
- Matrizes esparsas
- Solu√ß√µes aproximadas suficientes
- Efici√™ncia computacional

**Classifica√ß√£o dos M√©todos:**
1. **M√©todos Diretos:** Solu√ß√£o exata (em aritm√©tica exata)
   - Elimina√ß√£o de Gauss, Decomposi√ß√£o LU, QR
2. **M√©todos Iterativos:** Sequ√™ncia convergente
   - Jacobi, Gauss-Seidel, Gradiente Conjugado

---

## **2. üìä M√©todos Diretos**

### **2.1 Elimina√ß√£o de Gauss**

**Princ√≠pio:** Transformar A em matriz triangular superior.

**Processo:**
```
[A|b] ‚Üí [Triangular Superior|b']

Depois resolver por substitui√ß√£o retroativa
```

**Algoritmo:**
```
Para k = 1 at√© n-1:
  Para i = k+1 at√© n:
    multiplicador m·µ¢‚Çñ = a·µ¢‚Çñ / a‚Çñ‚Çñ
    Para j = k at√© n:
      a·µ¢‚±º = a·µ¢‚±º - m·µ¢‚Çñ √ó a‚Çñ‚±º
    b·µ¢ = b·µ¢ - m·µ¢‚Çñ √ó b‚Çñ
```

**Exemplo:**
```
Sistema:
x + 2y + z = 4
2x + y + z = 3
x + y + 2z = 5

Matriz aumentada:
[1  2  1 | 4]
[2  1  1 | 3]
[1  1  2 | 5]

Passo 1: Eliminar x da linha 2 e 3
m‚ÇÇ‚ÇÅ = 2/1 = 2
m‚ÇÉ‚ÇÅ = 1/1 = 1

[1  2   1  | 4]
[0 -3  -1  |-5]  (L2 - 2√óL1)
[0 -1   1  | 1]  (L3 - L1)

Passo 2: Eliminar y da linha 3
m‚ÇÉ‚ÇÇ = -1/(-3) = 1/3

[1  2    1   | 4]
[0 -3   -1   |-5]
[0  0   2/3  | 2/3]  (L3 - (1/3)√óL2)

Substitui√ß√£o Retroativa:
z = (2/3)/(2/3) = 1
y = (-5 - (-1)√ó1)/(-3) = 4/3
x = (4 - 2√ó(4/3) - 1) = 1/3

Solu√ß√£o: x = 1/3, y = 4/3, z = 1
```

**Complexidade:**
```
Elimina√ß√£o: O(n¬≥/3)
Substitui√ß√£o: O(n¬≤)
Total: O(n¬≥)
```

**Limita√ß√µes:**
- Piv√¥ zero causa divis√£o por zero
- Piv√¥s pequenos causam instabilidade num√©rica
- **Solu√ß√£o:** Pivoteamento parcial ou completo

### **2.2 Pivoteamento**

**Pivoteamento Parcial:**
```
Em cada etapa k, escolher piv√¥ a‚Çñ‚Çñ com maior |a·µ¢‚Çñ|
(trocar linhas se necess√°rio)
```

**Pivoteamento Completo:**
```
Escolher piv√¥ com maior |a·µ¢‚±º| em toda submatriz
(trocar linhas e colunas)
```

**Por que √© importante:**
```
Sem pivoteamento:
[10‚Åª‚Å¥  1] [x]   [1]
[1     1] [y] = [2]

Com arredamento:
x = 0 (errado!)

Com pivoteamento:
[1     1] [y]   [2]
[10‚Åª‚Å¥  1] [x] = [1]

x ‚âà 1, y ‚âà 1 (correto!)
```

### **2.3 Decomposi√ß√£o LU**

**Princ√≠pio:** Fatorar A = LU
```
L: matriz triangular inferior (Lower)
U: matriz triangular superior (Upper)
```

**Vantagem:** Resolver m√∫ltiplos sistemas com mesma A:
```
Ax = b‚ÇÅ  ‚Üí  LUx = b‚ÇÅ
Ax = b‚ÇÇ  ‚Üí  LUx = b‚ÇÇ
...

1. Fatorar A = LU uma vez: O(n¬≥)
2. Para cada b·µ¢: resolver Ly = b·µ¢ e Ux = y: O(n¬≤)
```

**Algoritmo de Doolittle:**
```
L tem diagonal de 1s
U √© obtida pela elimina√ß√£o de Gauss

Para k = 1 at√© n:
  u‚Çñ‚±º = a‚Çñ‚±º - Œ£(l‚Çñ‚Çö√óu‚Çö‚±º)  (j = k at√© n)
  l·µ¢‚Çñ = (a·µ¢‚Çñ - Œ£(l·µ¢‚Çö√óu‚Çö‚Çñ))/u‚Çñ‚Çñ  (i = k+1 at√© n)
```

**Exemplo:**
```
A = [2  1  1]
    [4  3  3]
    [8  7  9]

L = [1    0    0]
    [2    1    0]
    [4    3    1]

U = [2  1  1]
    [0  1  1]
    [0  0  2]

Verifica√ß√£o: L√óU = A ‚úì
```

**Variantes:**
- **Crout:** U tem diagonal de 1s
- **Cholesky:** Para matrizes sim√©tricas positivas definidas (A = LL·µÄ)

### **2.4 Decomposi√ß√£o QR**

**Princ√≠pio:** Fatorar A = QR
```
Q: matriz ortogonal (Q·µÄQ = I)
R: matriz triangular superior
```

**Vantagens:**
- **Mais est√°vel numericamente** que LU
- √ötil para problemas de m√≠nimos quadrados
- Funciona para matrizes retangulares

**M√©todos:**

**1. Gram-Schmidt:**
```
Ortogonaliza colunas de A sequencialmente

q·µ¢ = a·µ¢ - Œ£(a·µ¢·µÄq‚±º)q‚±º  (j < i)
q·µ¢ = q·µ¢ / ||q·µ¢||
```

**2. Reflex√µes de Householder:**
```
Usa matrizes de reflex√£o para zerar elementos abaixo da diagonal

H·µ¢ = I - 2v·µ¢v·µ¢·µÄ

Mais est√°vel que Gram-Schmidt
```

**3. Rota√ß√µes de Givens:**
```
Rotaciona pares de elementos para zerar um por vez

√ötil para matrizes esparsas
```

**Aplica√ß√£o em M√≠nimos Quadrados:**
```
Minimizar ||Ax - b||¬≤

Solu√ß√£o: x = (A·µÄA)‚Åª¬πA·µÄb

Com QR: A = QR
x = R‚Åª¬πQ·µÄb

Vantagem: N√£o precisa calcular A·µÄA (melhor condicionamento)
```

### **2.5 Decomposi√ß√£o SVD**

**Singular Value Decomposition:**
```
A = UŒ£V·µÄ

onde:
‚Ä¢ U: m√óm ortogonal (vetores singulares √† esquerda)
‚Ä¢ Œ£: m√ón diagonal (valores singulares œÉ·µ¢ ‚â• 0)
‚Ä¢ V: n√ón ortogonal (vetores singulares √† direita)
```

**Solu√ß√£o de M√≠nimos Quadrados:**
```
x = VŒ£‚Å∫U·µÄb

onde Œ£‚Å∫ √© pseudo-inversa:
Œ£‚Å∫·µ¢·µ¢ = 1/œÉ·µ¢  se œÉ·µ¢ ‚â† 0
      = 0     se œÉ·µ¢ = 0
```

**Vantagens:**
- **Mais est√°vel** de todos os m√©todos
- Funciona para qualquer matriz
- Detecta rank deficiency
- An√°lise de condicionamento

**Aplica√ß√µes:**
- PCA (An√°lise de Componentes Principais)
- Compress√£o de imagens
- Recomenda√ß√£o (Matrix Factorization)
- Redu√ß√£o de dimensionalidade

---

## **3. üîÑ M√©todos Iterativos**

### **3.1 Quando Usar M√©todos Iterativos?**

**Vantagens:**
- Eficientes para matrizes **grandes e esparsas**
- N√£o requerem armazenar matriz completa
- Podem parar com aproxima√ß√£o suficiente
- Paraleliz√°veis

**Desvantagens:**
- Podem n√£o convergir
- Converg√™ncia pode ser lenta
- Requerem bom chute inicial

### **3.2 M√©todo de Jacobi**

**Decomposi√ß√£o:** A = D + L + U
```
D: diagonal de A
L: triangular inferior estrita (abaixo da diagonal)
U: triangular superior estrita (acima da diagonal)
```

**Itera√ß√£o:**
```
Dx‚ÅΩ·µè‚Å∫¬π‚Åæ = b - (L + U)x‚ÅΩ·µè‚Åæ

ou elemento a elemento:

x·µ¢‚ÅΩ·µè‚Å∫¬π‚Åæ = (b·µ¢ - Œ£a·µ¢‚±ºx‚±º‚ÅΩ·µè‚Åæ) / a·µ¢·µ¢  (j ‚â† i)
```

**Exemplo:**
```
Sistema:
4x + y = 15
x + 3y = 14

Itera√ß√µes:
x‚ÅΩ‚Å∞‚Åæ = [0, 0]

x‚ÅΩ¬π‚Åæ = [(15 - 0)/4, (14 - 0)/3] = [3.75, 4.67]

x‚ÅΩ¬≤‚Åæ = [(15 - 4.67)/4, (14 - 3.75)/3] = [2.58, 3.42]

x‚ÅΩ¬≥‚Åæ = [(15 - 3.42)/4, (14 - 2.58)/3] = [2.90, 3.81]

...converge para x = 3, y = 4
```

**Converg√™ncia:**
```
Condi√ß√£o suficiente: A √© estritamente diagonal dominante

|a·µ¢·µ¢| > Œ£|a·µ¢‚±º|  (j ‚â† i) para todo i
```

### **3.3 M√©todo de Gauss-Seidel**

**Melhoria:** Usa valores **j√° atualizados** na mesma itera√ß√£o.

**Itera√ß√£o:**
```
x·µ¢‚ÅΩ·µè‚Å∫¬π‚Åæ = (b·µ¢ - Œ£a·µ¢‚±ºx‚±º‚ÅΩ·µè‚Å∫¬π‚Åæ - Œ£a·µ¢‚±ºx‚±º‚ÅΩ·µè‚Åæ) / a·µ¢·µ¢
              j<i           j>i

Usa x‚ÅΩ·µè‚Å∫¬π‚Åæ para j < i (j√° calculados)
Usa x‚ÅΩ·µè‚Åæ para j > i (ainda n√£o calculados)
```

**Forma Matricial:**
```
(D + L)x‚ÅΩ·µè‚Å∫¬π‚Åæ = b - Ux‚ÅΩ·µè‚Åæ
```

**Exemplo (mesmo sistema):**
```
x‚ÅΩ‚Å∞‚Åæ = [0, 0]

x‚ÇÅ‚ÅΩ¬π‚Åæ = (15 - 0)/4 = 3.75
x‚ÇÇ‚ÅΩ¬π‚Åæ = (14 - 3.75)/3 = 3.42  (usa x‚ÇÅ‚ÅΩ¬π‚Åæ!)

x‚ÇÅ‚ÅΩ¬≤‚Åæ = (15 - 3.42)/4 = 2.90
x‚ÇÇ‚ÅΩ¬≤‚Åæ = (14 - 2.90)/3 = 3.70

x‚ÇÅ‚ÅΩ¬≥‚Åæ = (15 - 3.70)/4 = 2.83
x‚ÇÇ‚ÅΩ¬≥‚Åæ = (14 - 2.83)/3 = 3.72

...converge mais r√°pido que Jacobi!
```

**Converg√™ncia:**
```
‚Ä¢ Mesma condi√ß√£o de Jacobi (diagonal dominante)
‚Ä¢ Geralmente converge mais r√°pido que Jacobi
‚Ä¢ Pode convergir quando Jacobi n√£o converge
```

### **3.4 M√©todo SOR (Successive Over-Relaxation)**

**Melhoria:** Adiciona fator de **relaxamento** œâ.

**Itera√ß√£o:**
```
xÃÉ·µ¢ = (b·µ¢ - Œ£a·µ¢‚±ºx‚±º‚ÅΩ·µè‚Å∫¬π‚Åæ - Œ£a·µ¢‚±ºx‚±º‚ÅΩ·µè‚Åæ) / a·µ¢·µ¢
     j<i           j>i

x·µ¢‚ÅΩ·µè‚Å∫¬π‚Åæ = œâ√óxÃÉ·µ¢ + (1-œâ)√óx·µ¢‚ÅΩ·µè‚Åæ

onde:
‚Ä¢ œâ = 1: Gauss-Seidel
‚Ä¢ 1 < œâ < 2: Over-relaxation (acelera)
‚Ä¢ 0 < œâ < 1: Under-relaxation (estabiliza)
```

**Escolha de œâ:**
```
‚Ä¢ Te√≥rica: Depende de propriedades de A
‚Ä¢ Pr√°tica: Experimenta√ß√£o (t√≠pico: 1.0-1.5)
‚Ä¢ √ìtimo: œâ_√≥timo ‚âà 2/(1 + ‚àö(1-œÅ¬≤))
  onde œÅ √© raio espectral de Gauss-Seidel
```

### **3.5 Gradiente Conjugado**

**Para:** Sistemas sim√©tricos positivos definidos (SPD).

**Princ√≠pio:** Minimizar fun√ß√£o quadr√°tica
```
f(x) = (1/2)x·µÄAx - b·µÄx

‚àáf(x) = Ax - b = 0  ‚Üí  Ax = b
```

**Algoritmo:**
```
x‚ÅΩ‚Å∞‚Åæ = chute inicial
r‚ÅΩ‚Å∞‚Åæ = b - Ax‚ÅΩ‚Å∞‚Åæ  (res√≠duo)
p‚ÅΩ‚Å∞‚Åæ = r‚ÅΩ‚Å∞‚Åæ  (dire√ß√£o)

Para k = 0, 1, 2, ...
  Œ±‚Çñ = (r‚ÅΩ·µè‚Åæ·µÄr‚ÅΩ·µè‚Åæ) / (p‚ÅΩ·µè‚Åæ·µÄAp‚ÅΩ·µè‚Åæ)
  x‚ÅΩ·µè‚Å∫¬π‚Åæ = x‚ÅΩ·µè‚Åæ + Œ±‚Çñp‚ÅΩ·µè‚Åæ
  r‚ÅΩ·µè‚Å∫¬π‚Åæ = r‚ÅΩ·µè‚Åæ - Œ±‚ÇñAp‚ÅΩ·µè‚Åæ
  Œ≤‚Çñ = (r‚ÅΩ·µè‚Å∫¬π‚Åæ·µÄr‚ÅΩ·µè‚Å∫¬π‚Åæ) / (r‚ÅΩ·µè‚Åæ·µÄr‚ÅΩ·µè‚Åæ)
  p‚ÅΩ·µè‚Å∫¬π‚Åæ = r‚ÅΩ·µè‚Å∫¬π‚Åæ + Œ≤‚Çñp‚ÅΩ·µè‚Åæ
```

**Propriedades:**
- **Teoricamente:** Converge em no m√°ximo n itera√ß√µes
- **Praticamente:** Aproxima√ß√£o boa em muito menos itera√ß√µes
- **Eficiente:** Apenas produto matriz-vetor por itera√ß√£o
- **Ideal:** Para matrizes grandes e esparsas

**Pr√©-condicionamento:**
```
Resolver M‚Åª¬πAx = M‚Åª¬πb

onde M √© matriz de pr√©-condicionamento:
‚Ä¢ M ‚âà A (aproxima A)
‚Ä¢ M f√°cil de inverter
‚Ä¢ Melhora condicionamento

Exemplo: M = diagonal de A
```

---

## **4. üìä An√°lise de Erros e Condicionamento**

### **4.1 Tipos de Erro**

**Erro de Arredondamento:**
```
Computadores t√™m precis√£o finita
Opera√ß√µes introduzem pequenos erros
Erros se acumulam
```

**Erro de Truncamento:**
```
M√©todos iterativos param antes da converg√™ncia
```

**Erro Total:**
```
||x_exato - x_computado||
```

### **4.2 N√∫mero de Condi√ß√£o**

**Defini√ß√£o:**
```
Œ∫(A) = ||A|| √ó ||A‚Åª¬π||

Para norma-2:
Œ∫‚ÇÇ(A) = œÉ‚Çò‚Çê‚Çì / œÉ‚Çò·µ¢‚Çô
```

**Interpreta√ß√£o:**
```
Œ∫(A) ‚âà 1:     Bem-condicionado
Œ∫(A) ‚âà 10¬≥:   Mal-condicionado
Œ∫(A) ‚âà 10‚Å∂:   Muito mal-condicionado
```

**Amplifica√ß√£o de Erro:**
```
Perturba√ß√£o em b:
||Œîx|| / ||x|| ‚â§ Œ∫(A) √ó (||Œîb|| / ||b||)

Se Œ∫(A) = 10‚Å∂ e erro em b √© 10‚Åª‚Å∏:
Erro em x pode ser 10‚Åª¬≤!
```

**Exemplo:**
```
A = [1    1  ]    Œ∫(A) ‚âà 4 (bem-condicionado)
    [0  0.01]

B = [1    1   ]   Œ∫(B) ‚âà 4√ó10‚Å¥ (mal-condicionado)
    [0  10‚Åª‚Åµ]
```

### **4.3 Normas Vetoriais e Matriciais**

**Normas Vetoriais:**
```
||x||‚ÇÅ = Œ£|x·µ¢|               (norma-1)
||x||‚ÇÇ = ‚àö(Œ£x·µ¢¬≤)             (norma-2, euclidiana)
||x||‚àû = max|x·µ¢|             (norma-infinito)
```

**Normas Matriciais:**
```
||A||‚ÇÅ = max_j Œ£|a·µ¢‚±º|        (m√°ximo da soma das colunas)
||A||‚ÇÇ = ‚àö(Œª‚Çò‚Çê‚Çì(A·µÄA))        (maior valor singular)
||A||‚àû = max_i Œ£|a·µ¢‚±º|        (m√°ximo da soma das linhas)
||A||F = ‚àö(Œ£Œ£a·µ¢‚±º¬≤)           (Frobenius)
```

### **4.4 Crit√©rios de Parada**

**Para M√©todos Iterativos:**

**1. Res√≠duo:**
```
||Ax‚ÅΩ·µè‚Åæ - b|| < tol
```

**2. Mudan√ßa Relativa:**
```
||x‚ÅΩ·µè‚Å∫¬π‚Åæ - x‚ÅΩ·µè‚Åæ|| / ||x‚ÅΩ·µè‚Å∫¬π‚Åæ|| < tol
```

**3. Combinado:**
```
(||Ax‚ÅΩ·µè‚Åæ - b|| < tol‚ÇÅ) E (||x‚ÅΩ·µè‚Å∫¬π‚Åæ - x‚ÅΩ·µè‚Åæ|| < tol‚ÇÇ)
```

**4. N√∫mero M√°ximo de Itera√ß√µes:**
```
k > k_max
```

---

## **5. üéØ Matrizes Especiais**

### **5.1 Matrizes Esparsas**

**Defini√ß√£o:** Maioria dos elementos s√£o zero.

**Armazenamento Eficiente:**

**COO (Coordinate):**
```
Armazena: (linha, coluna, valor) para elementos n√£o-zeros
```

**CSR (Compressed Sparse Row):**
```
Armazena: valores, √≠ndices de colunas, ponteiros de linhas
Eficiente para opera√ß√µes por linha
```

**CSC (Compressed Sparse Column):**
```
Similar a CSR, mas por coluna
Eficiente para opera√ß√µes por coluna
```

**Exemplo:**
```
A = [1  0  0  2]
    [0  3  0  0]
    [4  0  5  0]
    [0  6  0  7]

COO:
rows = [0, 0, 1, 2, 2, 3, 3]
cols = [0, 3, 1, 0, 2, 1, 3]
vals = [1, 2, 3, 4, 5, 6, 7]
```

**M√©todos Especiais:**
- Elimina√ß√£o de Gauss com fill-in m√≠nimo
- M√©todos iterativos (muito eficientes!)
- Fatora√ß√£o Cholesky esparsa

### **5.2 Matrizes Banda**

**Defini√ß√£o:** Elementos n√£o-zeros concentrados perto da diagonal.

**Largura de Banda:**
```
b = max(|i-j|) para a·µ¢‚±º ‚â† 0
```

**Exemplos:**
- **Tridiagonal:** b = 1
- **Pentadiagonal:** b = 2

**Vantagem:** Algoritmos O(n√ób¬≤) em vez de O(n¬≥).

**Aplica√ß√µes:**
- Diferen√ßas finitas (EDPs)
- Splines
- S√©ries temporais (AR, MA)

### **5.3 Matrizes Sim√©tricas Positivas Definidas**

**Propriedades:**
- A = A·µÄ (sim√©trica)
- x·µÄAx > 0 para todo x ‚â† 0 (positiva definida)
- Autovalores positivos
- Decomposi√ß√£o de Cholesky existe

**Fatora√ß√£o de Cholesky:**
```
A = LL·µÄ

onde L √© triangular inferior

Vantagens:
‚Ä¢ Metade do custo de LU
‚Ä¢ Numericamente est√°vel
‚Ä¢ √önica (com diagonal de L positiva)
```

**Algoritmo:**
```
Para k = 1 at√© n:
  l‚Çñ‚Çñ = ‚àö(a‚Çñ‚Çñ - Œ£l‚Çñ‚±º¬≤)  (j < k)
  l·µ¢‚Çñ = (a·µ¢‚Çñ - Œ£l·µ¢‚±ºl‚Çñ‚±º) / l‚Çñ‚Çñ  (i > k, j < k)
```

**Aplica√ß√µes:**
- M√≠nimos quadrados
- Estat√≠stica (matrizes de covari√¢ncia)
- Otimiza√ß√£o
- Processos gaussianos

---

## **6. üöÄ Aplica√ß√µes**

### **6.1 Regress√£o Linear**

**Problema:** Minimizar ||Ax - b||¬≤

**Equa√ß√µes Normais:**
```
A·µÄAx = A·µÄb

Resolver com:
‚Ä¢ Decomposi√ß√£o de Cholesky (se A·µÄA √© SPD)
‚Ä¢ Decomposi√ß√£o QR (mais est√°vel)
‚Ä¢ SVD (mais robusto)
```

### **6.2 Redes El√©tricas**

**Lei de Kirchhoff:** Soma de correntes em n√≥ = 0

**Sistema:**
```
Gv = i

onde:
‚Ä¢ G: matriz de condut√¢ncia
‚Ä¢ v: tens√µes nos n√≥s
‚Ä¢ i: correntes injetadas
```

**Caracter√≠sticas:**
- G √© esparsa (conex√µes locais)
- G √© sim√©trica
- G √© positiva definida

### **6.3 Diferen√ßas Finitas (EDPs)**

**Equa√ß√£o de Laplace:** ‚àá¬≤u = 0

**Discretiza√ß√£o:**
```
(u·µ¢‚Çä‚ÇÅ,‚±º - 2u·µ¢,‚±º + u·µ¢‚Çã‚ÇÅ,‚±º)/h¬≤ + 
(u·µ¢,‚±º‚Çä‚ÇÅ - 2u·µ¢,‚±º + u·µ¢,‚±º‚Çã‚ÇÅ)/h¬≤ = 0
```

**Sistema Linear:**
```
Au = f

onde A √© esparsa e banda
```

### **6.4 PageRank (Google)**

**Problema:** Calcular import√¢ncia de p√°ginas web.

**Sistema:**
```
(I - Œ±P)x = (1-Œ±)e/n

onde:
‚Ä¢ P: matriz de transi√ß√£o (esparsa!)
‚Ä¢ Œ±: damping factor (‚âà 0.85)
‚Ä¢ x: vetor de PageRank
```

**Solu√ß√£o:** M√©todo iterativo (Power Method)

### **6.5 Machine Learning**

**Treinamento de Redes Neurais:**
```
Hessiano Œîw = -‚àáL

onde H √© matriz Hessiana
Resolver para Œîw (atualiza√ß√£o de pesos)
```

**M√©todos:**
- Gradiente Conjugado
- L-BFGS (quasi-Newton)
- Adam (gradiente adaptativo)

---

## **7. üßÆ Exerc√≠cios Resolvidos**

### **Exerc√≠cio 1: Elimina√ß√£o de Gauss**
**Sistema:**
```
x + y = 3
2x + 3y = 8
```

**Solu√ß√£o:**
```
[1  1 | 3]
[2  3 | 8]

Eliminar x da L2:
[1  1 | 3]
[0  1 | 2]  (L2 - 2√óL1)

Substitui√ß√£o:
y = 2
x = 3 - 2 = 1

Solu√ß√£o: x = 1, y = 2
```

### **Exerc√≠cio 2: Jacobi**
**Sistema:**
```
4x + y = 15
x + 3y = 14
```

**Solu√ß√£o:**
```
x‚ÅΩ‚Å∞‚Åæ = [0, 0]

Itera√ß√£o 1:
x‚ÅΩ¬π‚Åæ = (15 - 0)/4 = 3.75
y‚ÅΩ¬π‚Åæ = (14 - 0)/3 = 4.67

Itera√ß√£o 2:
x‚ÅΩ¬≤‚Åæ = (15 - 4.67)/4 = 2.58
y‚ÅΩ¬≤‚Åæ = (14 - 3.75)/3 = 3.42

...continua at√© convergir para x = 3, y = 4
```

### **Exerc√≠cio 3: Condicionamento**
**Calcular Œ∫(A) para:**
```
A = [1  2]
    [2  4.001]

Valores singulares:
œÉ‚ÇÅ ‚âà 4.5
œÉ‚ÇÇ ‚âà 0.001

Œ∫(A) = œÉ‚ÇÅ/œÉ‚ÇÇ ‚âà 4500

Mal-condicionado! Pequenos erros se amplificam.
```

---

## **8. üíª Implementa√ß√£o em Python**

```python
import numpy as np
from scipy import linalg
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import spsolve, cg
import matplotlib.pyplot as plt

# Sistema de exemplo
A = np.array([[4, 1, 0],
              [1, 3, 1],
              [0, 1, 2]], dtype=float)
b = np.array([15, 14, 8], dtype=float)

print("Sistema Ax = b:")
print(f"A =\n{A}")
print(f"b = {b}")

# 1. Elimina√ß√£o de Gauss (direto)
x_gauss = linalg.solve(A, b)
print(f"\n1. Gauss: x = {x_gauss}")

# 2. Decomposi√ß√£o LU
P, L, U = linalg.lu(A)
print(f"\n2. LU Decomposition:")
print(f"L =\n{L}")
print(f"U =\n{U}")
y = linalg.solve(L, P @ b)
x_lu = linalg.solve(U, y)
print(f"Solu√ß√£o: x = {x_lu}")

# 3. Decomposi√ß√£o QR
Q, R = linalg.qr(A)
x_qr = linalg.solve(R, Q.T @ b)
print(f"\n3. QR: x = {x_qr}")

# 4. Decomposi√ß√£o SVD
U_svd, s, Vt = linalg.svd(A)
x_svd = Vt.T @ np.diag(1/s) @ U_svd.T @ b
print(f"\n4. SVD: x = {x_svd}")

# 5. Cholesky (A √© SPD)
L_chol = linalg.cholesky(A, lower=True)
y_chol = linalg.solve(L_chol, b)
x_chol = linalg.solve(L_chol.T, y_chol)
print(f"\n5. Cholesky: x = {x_chol}")

# 6. M√©todo de Jacobi
def jacobi(A, b, x0=None, tol=1e-6, max_iter=100):
    n = len(b)
    x = np.zeros(n) if x0 is None else x0.copy()
    D = np.diag(A)
    R = A - np.diagflat(D)
    
    for i in range(max_iter):
        x_new = (b - R @ x) / D
        if np.linalg.norm(x_new - x) < tol:
            return x_new, i+1
        x = x_new
    return x, max_iter

x_jacobi, iters = jacobi(A, b)
print(f"\n6. Jacobi: x = {x_jacobi} (itera√ß√µes: {iters})")

# 7. M√©todo de Gauss-Seidel
def gauss_seidel(A, b, x0=None, tol=1e-6, max_iter=100):
    n = len(b)
    x = np.zeros(n) if x0 is None else x0.copy()
    
    for k in range(max_iter):
        x_old = x.copy()
        for i in range(n):
            x[i] = (b[i] - A[i,:i] @ x[:i] - A[i,i+1:] @ x_old[i+1:]) / A[i,i]
        if np.linalg.norm(x - x_old) < tol:
            return x, k+1
        x = x
    return x, max_iter

x_gs, iters_gs = gauss_seidel(A, b)
print(f"\n7. Gauss-Seidel: x = {x_gs} (itera√ß√µes: {iters_gs})")

# 8. Gradiente Conjugado
x_cg, info = cg(A, b, tol=1e-6)
print(f"\n8. Gradiente Conjugado: x = {x_cg}")

# 9. An√°lise de Condicionamento
cond = np.linalg.cond(A)
print(f"\n9. N√∫mero de Condi√ß√£o: Œ∫(A) = {cond:.2f}")

# 10. Verifica√ß√£o
residual = b - A @ x_gauss
print(f"\n10. Res√≠duo: ||Ax - b|| = {np.linalg.norm(residual):.2e}")

# 11. Matriz Esparsa
A_sparse = csr_matrix(A)
x_sparse = spsolve(A_sparse, b)
print(f"\n11. Esparsa: x = {x_sparse}")

# 12. Visualiza√ß√£o da Converg√™ncia
def convergence_plot():
    x_true = linalg.solve(A, b)
    errors_jacobi = []
    errors_gs = []
    
    x_j = np.zeros(len(b))
    x_g = np.zeros(len(b))
    
    for i in range(30):
        # Jacobi
        D = np.diag(A)
        R = A - np.diagflat(D)
        x_j = (b - R @ x_j) / D
        errors_jacobi.append(np.linalg.norm(x_j - x_true))
        
        # Gauss-Seidel
        for j in range(len(b)):
            x_g[j] = (b[j] - A[j,:j] @ x_g[:j] - A[j,j+1:] @ x_g[j+1:]) / A[j,j]
        errors_gs.append(np.linalg.norm(x_g - x_true))
    
    plt.figure(figsize=(10, 6))
    plt.semilogy(errors_jacobi, 'o-', label='Jacobi')
    plt.semilogy(errors_gs, 's-', label='Gauss-Seidel')
    plt.xlabel('Itera√ß√£o')
    plt.ylabel('Erro ||x - x_true||')
    plt.title('Converg√™ncia dos M√©todos Iterativos')
    plt.legend()
    plt.grid(True)
    plt.show()

convergence_plot()
```

---

## **9. üîó Recursos Adicionais**

### **Livros Recomendados**
- **Numerical Linear Algebra** - Trefethen & Bau
- **Matrix Computations** - Golub & Van Loan
- **Numerical Analysis** - Burden & Faires
- **Applied Numerical Linear Algebra** - Demmel

### **Cursos Online**
- MIT 18.06 - Linear Algebra (Gilbert Strang)
- Stanford CS 205A - Mathematical Methods
- Coursera - Numerical Methods

### **Bibliotecas Python**
```python
# B√°sico
import numpy as np
import scipy.linalg

# Esparso
from scipy.sparse import csr_matrix, linalg as sparse_linalg
from scipy.sparse.linalg import spsolve, cg, gmres, bicgstab

# Especializadas
import pyamg  # Multigrid alg√©brico
import petsc4py  # PETSc (High Performance)
```

### **Ferramentas**
- [Matrix Calculator](https://matrixcalc.org/)
- [Wolfram Alpha](https://www.wolframalpha.com/)
- [Octave/MATLAB](https://www.gnu.org/software/octave/)

---

**Voltar para:** [Estat√≠stica](../README.md) | [Notebooks](../../README.md)
